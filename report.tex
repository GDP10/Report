\documentclass[a4paper, 12pt]{article}
\usepackage[backend=biber]{biblatex}
\usepackage{graphicx}
\usepackage{caption}

%Package stuff for the user stories
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing,calc}
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate 
     }}

     % Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border] 
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}    
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}   
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) -- 
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white, 
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

%End User story package stuff

\addbibresource{report.bib}
\nocite{*}

\newcommand{\userstory}[4]{
	\begin{parchment}[User Story: #1]
	\noindent\emph{\textbf{AS A}} {\small #2}\\*\emph{\textbf{I WANT}} {\small #3}\\*\emph{\textbf{SO THAT}} {\small #4}
	\end{parchment}
}

\begin{document}

\begin{titlepage}

	\center

	{\large ELECTRONICS AND COMPUTER SCIENCE}\\[0.2cm]
	{\large FACULTY OF PHYSICAL SCIENCES AND ENGINEERING}\\[0.2cm]
	{\large UNIVERSITY OF SOUTHAMPTON}\\[3cm]

	{\Large Peter Prince}\\[0.2cm]
	{\Large James Robinson}\\[0.2cm]
	{\Large Charles Sherman}\\[0.2cm]
	{\Large Andrew Sullivan}\\[1cm]
	{\Large \today}\\[3cm]

	{\LARGE Aeronautical Event Service}\\[3cm]

	{\large Project Supervisor: Dr Robert Walters}\\[0.2cm]
	{\large Second Examiner: -}\\[2.5cm]

	{\large A project report submitted for the award of}\\[0.2cm]
	{\Large MEng Computer Science}

\end{titlepage}

\begin{abstract}

\end{abstract}

\newpage

\section*{Acknowledgements}

\newpage

\tableofcontents
\newpage

\listoffigures
\newpage

\section{Introduction}
\label{sec:introduction}

\subsection{Project Description}
\subsubsection{Motivation}
Pilots need to be notified of hazards and events that may require them to make changes to their flight plan. Examples of such events includes: closed runways, restricted airspaces, extreme weather conditions and even the presence of migrating birds through an airspace. Currently, flight planners receive such information by pulling 'NOTAM's, or 'Notice to Airmen', and must manually sort through all NOTAMs to find only the notices which are relevant to their flight, which is extremely time consuming. The efficiency of this task could be improved dramatically if the filtering were performed automatically.

\subsubsection{Customer}

The customer for this project was Snowflake Software, a company focused on open data exchange, and various products based on mapping data. Within sprint meetings the company was represented by Eddie Curtis, who expressed the feature requests and gave feedback, representing Snowflake as a whole.

\subsubsection{Goals}

The vision, as stated by Snowflake, was ``To develop a proof of concept push event service for delivering digital NOTAM updates to pilots; to demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. From this vision, the goal of the software produced over the course of the project was to explore options and find methods to limit the incoming data to a flight planner to only relevant data by utilising a publish and subscribe model, removing the need for manual filtering and the pulling of relevant NOTAMs by pilots. 

The subscriptions to this service had to be based on the relevancy of NOTAMs to the recipients. This relevancy should be based on simple factors, such as departure and arrival airports, and more complex temporal queries, for example, limiting NOTAMs events to ones relevant only during the course of the flight. The software was required to use open standards for the publish and subscribe architecture, for example: OASIS Web Services Notification or WSN. The software also had to be capable of handling an estimated 10,000 messages per day. These messages come from two sources: the Federal NOTAM Service, which publishes a baseline NOTAM every 28 days, and updated delta files which arrive every 15 minutes. 

\subsection{Background Information}

\subsubsection{NOTAMs}
A `notice to airmen' (NOTAM), is a notice given to airmen about events in a particular airspace or at a particular airport. The events may be hazards that have to be avoided, such as dangerous weather or updates on runway closures, which may or may not be relevant to the flight. They're used by pilots and flight planners to notify them about events which have the potential to affect the safety of their flight. NOTAMs are distributed by various aviation authorities around the world and are generally received by pilots and flight planners by pull notifications.

The NOTAMs are delivered as deltas from a baseline of data. For instance, a pilot may be told in a NOTAM that runway 3 has been closed at their destination. They were previously told about the number of alternative runways at the destination airport in a baseline message. Deltas can be either temporary or non-temporary; temporary updates apply for a certain time frame whereas non-temporary updates last indefinitely. If there are many delta updates, it is often useful to make a snapshot. A snapshot is an updated baseline that is formed from the original baseline and a combination of deltas, this allows flight planners requesting a NOTAM after a large number of delta updates to quickly receive up to date information without having to request large numbers of delta updates.

All NOTAMs are sent as XML documents using a schema known as AIXM. The AIXM, or \emph{Aeronautical Information Exchange Model}, is a specification for aeronautical information flows and includes two major components: the AIXM conceptual model and its corresponding XML schema. The conceptual model is used to model all important properties, features and restrictions which govern all aeronautical information. Features include runways, aerodromes and airspaces, whereas properties can include information such as the width and length of a specific runway. This model is implemented as the AIXM XML schema which all NOTAMs are created using.

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth}
\includegraphics[scale=0.5]{aixmUML.png}
\end{center}
\caption{UML diagram highlighting some features of the AIXM conceptual model.\cite{aixm}}
\label{fig:example_net}
\end{figure}

\newpage

\subsubsection{Publish/Subscribe Architecture}

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth}
\input{broker.pdf_tex}
\end{center}
\caption{Publish/Subscribe Architecture with Broker}
\label{fig:example_net}
\end{figure}

Publish-subscribe is a messaging architecture that allows decoupling between the \emph{producers} and the \emph{consumers} of \emph{messages}. Messages are categorised into \emph{topics}; producers publish messages on a given topic with no knowledge of which (if any) subscribers the topic has, and consumers subscribe to a given topic with no knowledge of which (if any) producers are publishing messages with that topic.

This architecture is generally implemented using a \emph{broker}, which exists as middleware between the producers and consumers. Producers send messages on a given topic to the broker, which then decides which consumers the messages should be forwarded to. Consumers subscribe via the broker to topics they’re interested in, and the broker forwards relevant messages to them. This architecture has a number of benefits, including providing loose coupling between producers and consumers (since they need no knowledge of each other to make use of this architecture) and facilitating scalability by supporting arbitrary numbers of producers and consumers for any given topic.

In Java, brokers typically implement the Java Message Service (JMS) API provided by Java Enterprise Edition, which offers a consistent interface for communicating with many different message broker implementations.

\subsubsection{Web Services Notification}

Web services notification (or WSN) is a set of specifications created by OASIS that defines the interactions between web services, using notifications, which can form the basis of a publish/subscribe architecture built using web services. They provide a standardized way to communicate with and between web services, without having prior information about them. It includes facilities for detailed subscription, where those wishing to consume information can register with a web service along with details about the type of information they wish to receive. WSN was designed to revolve around these subscription details known as ``topics'' and use them to filter down subscriptions, allowing for large scale usage of the publish/subscribe architecture without unwanted message overload.

WSN covers a family of smaller specifications including WS-Base Notifications, WS-Brokered Notification and WS-Topics. WS-Base Notifications is a specification of the simple producer/consumer relationship, consumers send out subscription requests to the producers, specifying the subset of messages produced by that producer which it wishes to receive. The request also contains a reference to the consumer itself, so the producer knows where to send the messages.

WS-Brokered Notification works in a similar manner to WS-Base, but introduces a middleman between the message publishers and the message subscribers known as the broker. The broker is designed to decouple the connection between producers and consumers and allows for advanced messaging features such as demand-based publishing, load-balancing and publication of messages from entities which are not themselves web service providers.

WS-Topics is a specification which defines a method for categorising subscriptions into ``topics''. This specification is designed to work in conjunction with WS-Base or WS-Brokered and contains various methods for defining groups of subscriptions.

\subsubsection{Web Services Description Language}

The Web Services Description Language (or WSDL) is an XML based language for describing the interfaces and functionality of a web service. A WSDL file provides a machine readable description of a web service's interface, including the function calls, parameters they expect and return values.\\
Typically, it is used in combination with SOAP to provide a web service, where a client program will read the WSDL, to determine the functions available, and then use SOAP to call them.

\subsubsection{JAXB and DOM Parsing}

JAXB, or ``Java Architecture for XML Binding'', is a tool used to marshal Java objects into XML and unmarshal XML into Java objects. It can be used as an alternative to standard XML parsers, taking complicated schema and converting XML created from them into usable Java objects. Use of JAXB was explored towards the end of the project for use in spatial filtering. We intended to use JAXB to parse XML filters of the OpenGIS standard in order to create filters within our system, however implementing JAXB proved beyond the scope of the project in terms of time, given the nature of the system as a proof of concept piece with a strict time constraint. Instead, we settled for a more commonly used XML parsing method known as ``Document Object Model'' or DOM parsing.

DOM parsing is one of the two most commonly used XML parsing methods, the other being SAX parsing. A DOM parser works by loading the entirety of an XML document into memory, modelling it as a tree to allow it to be easily traversed. The tree is traversed node by node to obtain the required information, in this case known tags within the OpenGIS standard for use in spatial filtering. This method of parsing generally has quite a large overhead when parsing large XML files, due to the fact it needs to parse the entire file.

\subsubsection{JIRA Issue Tracking}

Issue trackers are systems which maintains lists of issues in an ongoing project, with the ability to assign these issues to specific group members, report bugs while working and connect solved issues to source control commits which fix them. Issues can also be broken into sub-tasks, prioritised and categorised to improve efficiency. For example, many issues were recorded after each sprint meeting, these included work for the next sprint, work for near future sprints and overall project goals. These issue reports were categorised according to the sprint which they were to be completed in. When every issue attached to a sprint was marked as solved, a sprint was considered complete in terms of work.

As our customer is a company which regularly works on projects of a much greater scale than that of our AES system, they possess a number of professional tools for organisation and issue tracking. One of these is JIRA, an issue tracker created by Atlassian designed to work with our chosen cycle of creating user stories and acceptance criteria with the customer and assigning these stories to team members after each sprint meeting. As JIRA is a professional piece of software which normally requires a monthly fee for usage, we had to gain access to the server running it licensed to our customer through their VPN. By using the tools in use by our customer, they were able to easily keep track of progress and make comments on submissions to the service.

\subsubsection{ActiveMQ}

Apache ActiveMQ is an open source message queue service which communicates using the Java Message Service (JMS). It contains a number of features deemed useful to the project including subscription management and filtering. The filtering itself is through objects known as selectors, which use the SQL 92 syntax to filter messages. SQL 92 is a fairly barebones SQL syntax, which features basic arithmetic operators (addition, subtraction, multiplication, division and modulo), enough for the requirements set out by the customer of temporal filtering and spatial filtering using GPS co-ordinates.

\subsubsection{Git and GitHub}

Git is a distributed version control system, which allows multiple people to work on a single project at the same time, without necessitating the use of a shared network, as well as tracking document history, affording reversions or rollbacks to previous versions. Some form of source control is essential for effective collaboration between programmers and greatly aides coordination and communication.

GitHub is a Git repository hosting service, which also offers other features that aide collaborative coding, such as issue tracking and task assignment.

\subsubsection{Travis CI}

Travis CI is a free, hosted, continuous integration platform that is used to automatically build and test projects hosted on GitHub. It automatically detects when a commit has been pushed to a linked GitHub repository and attempts to build all branches and run tests. It can then be set up to show the results of this in the GitHub readme or to notify the developer in some other way, such as by emailing the results.

This practice of continues integration allows developers to detect problems quickly. This also makes fixing them easier, as less back tracking is required to identify when and where the issue occurred, reducing the number of large scale revisions required. It also means they can assure the software will run on a variety of computer set-ups, without time consuming manual testing.

\subsubsection{Cucumber}

Cucumber is a testing suite that runs behaviour-driven development style acceptance tests, i.e., tests written to check if the requirements of a specification are met by the code. These focus on testing behaviours, i.e. the action the code is supposed to perform, where each test goal is independent of the code’s structure, rather than an approach like unit testing, that tests individual units of code.
A single Cucumber test consists of a plain text feature definition, written in plain English, and corresponding step definitions, that is, the code that actually tests the feature. 

A feature definition describes a single behaviour the code is supposed to be able to perform (such as "Feature: Filter NOTAMs by time") and then outlines one or many scenarios: sets of conditions that test a facet of the behaviour. These describe initial conditions, steps that are then carried out, and what the results of performing those steps (with those initial conditions) should be. These are specified with the keywords "Given, When, Then", i.e., given a set of initials conditions, when something happens, then the program should do something (with the ability to have multiple steps with the keyword And). 

For each feature, there is then a set of corresponding step definitions, that is, code to test the feature, in the form of a subroutine for each step (e.g. there will be a single subroutine for "Given a broker is running", which will check that a broker is running and, if not, launch one). Depending on the keyword, these should either set up the initial conditions (in the case of Given or a following And), perform a particular action (in the case of When or a following And), or assert that particular conditions are true (in the case of Then or a following And). If all conditions are satisfied, the test passes, otherwise it fails.

\subsubsection{JSON}

JSON (or JavaScript Object Notation) is a human readable data interchange format and an alternative to XML. It transmits data objects consisting of attribute-value pairs and is primarily used for server-web application communication. One of its primary advantages is its relative concision and lack of redundancy when compared to XML.

\subsubsection{SOAP}

SOAP (or, originally, Simple Object Access Protocol) is a platform independent protocol for exchanging information between web services, based on XML, and is dependant on other application layer protocols for message transmission (typically HTTP but other transport protocols are supported as well). 

\subsubsection{Netcat}

Netcat is a utility to read from and write across network connections using TCP or UDP. It is designed so that it can easily and reliably be used as a back-end tool, so it can be used by other programs. It is frequently referred to as the 'Swiss army knife' of networking tool because of its wide array of features, such as port scanning and a large number of network debugging and investigation tools.

\subsubsection{cURL}

cURL is both a library and command line tool for sending and retrieving data using a wide variety of protocols (such as HTTP, FTP, SCP, POP3, etc.). It is very portable, working on a wide variety of platforms, and bindings are available for a large number of programming languages.


\newpage

\section{Planning and Organisation}
\label{sec:planning}

The project was built using the Agile software development practices of Scrum, and each sprint followed a similar structure. Every other week, our team would meet in  First, the customer came up with ideas for features which could be worked on in the following sprint. Next, user stories based on these features were created, dividing up the requirements of each feature and helping to create acceptance criteria which would come together to prove acceptance that the feature was successfully implemented.

At the beginning of each sprint, a sprint meeting was held to both close the previous sprint and prepare for the following one. First, features implemented in the previous sprint were demonstrated to the customer, allowing them to track the progress of the project and decide whether the feature had been successfully implemented. In the event the customer was not satisfied with the work done in the sprint, the user stories were kept in the backlog for the following sprint. Otherwise, the customer discussed future features to be implemented and broke the feature up into user stories to be added to the backlog.

Meetings also regularly featured exercises to identify issues and find solutions to problems faced by the group over the previous sprint. Before moving onto work that needed to be done for the following sprint, each group member and the customer filled in a table of three columns on a whiteboard. These columns listed points we felt happy about in terms of project progression, points we were unsure about and points we felt required discussion. This list was then prioritised and discussed in order, finding solutions for problems such as group dynamic issues and finding ways to continue with things we agreed were going well. These exercises not only created a more open forum for members of the group to start discussions about things they would otherwise be uncomfortable starting, but allowed the customer to be made aware of aspects of the project which were working well and we were proud of.

Various software was used to carry out this process, including JIRA (access to which was provided by the client) and Cucumber. JIRA is a tracker that manages user stories and their corresponding acceptance criteria as per each feature. User stories within JIRA are stored in the standardised \emph{``As a, I want, So that''} format, with their acceptance criteria written as \emph{``Given, When, Then''}. This format is also used by Cucumber for acceptance tests. Team members on JIRA can choose what user stories to complete or they can be assigned.

Early on in the project we were given access to Jira on Snowflake's private servers. This allowed us to keep control over our backlog and keep a record of what had been completed. However over the course of the project, the number of steps required to connect to the Snowflake VPN and access Jira as well as the difficulty from never having used the service before, we decided keeping track of the project would be better done through Github, which was already being used as a code repository. The issue tracking features within Github replaced Jira's extensive features (the majority of which far surpassed the requirements of a small project such as ours) and all tasks became issues, still using the standard user story format.

\subsection{Testing Methodology}

Behavioural-driven development style acceptance testing was used, using Cucumber to express the acceptance criteria for each story, allowing them to be converted into automated acceptance tests which could be run using JUnit. The scenarios were written such that the implementation of the feature can change without having to rewrite the file. For each story, when all acceptance tests passed within JUnit, the user story is removed from the backlog. This type of testing keeps testing focused on user stories, testing features rather than abstract units of code, better ensuring that the code satisfies the client's requirements, rather than some abstract measure of performance. 

Continuous integration was also used, utilizing Travis CI. This helps to ensure new code doesn't break old tests, ensures the software can run on a variety of computer set-ups, and allows problems to be detected more quickly and fixed with less backtracking.


\newpage

\section{Implementation}
\subsection{Sprint 1}

In the meeting we had with Snowflake, we were introduced to the project and we discussed the scope, requirements and overall goals of the project. 

\subsubsection{Sprint Goals}

For the first sprint the goal was to have a basic push notification system running that could publish and receive an XML. As well as this we had to set up our development environment and tools for the project.

We decided to program using Java as it was what our group had used most in the past. For version control, we set up a repository on Github~\ref{github}.

For this sprint we had a single user story as well as the epic that defined the whole project. We used Jira\ref{jira} to manage our user stories.

\userstory{Basic Push Notification}{flight planner}{to be notified if there is a change to my destination airport for my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

To fufill this goal we had to set up Apache ActiveMQ\ref{activeMQ} and find a way to give the broker a message and a way of displaying the message so we know it had been received. 

\subsubsection{Work Allocation}

As the sprint goal was very simple it was difficult to separate the work between team members. James Robinson did the majority of the coding work while everyone else in the group assisted by researching the possible tools we could use. This was in effect an extension of pair-programming, where two people working on a single machine take turns in programming while the other checks for mistakes.

Once we had found a solution that worked, James presented the code to everyone else so that the group understood which of the implementations we went with and why.

Apart from programming, James also helped to set up Maven on the team member's machines so that they could also work on the code. Charles Sherman set up the github repository to store the code for the project and Peter Prince typed up the user stories into Jira.

\subsubsection{Implementation}

After working on a basic implementation of ActiveMQ and exploring the documentation available online, it became obvious that it contained no implementation of WS-Notification and the page within its online documentation which was supposed to detail its implementation was a dead link. One of the requirements stated by the customer was the inclusion of WS-Notification and while this was a soft requirement, meaning that it's inclusion could be forgone if sufficient justification was given, the project was in an early enough stage at this point in development that it was worth exploring other alternatives. Alternatives such as developing our own WS-Notification implementation using CXF.

Apache CXF is an open source framework for producing web services, and after ActiveMQ was deemed unusable, research began into using CXF. Work started by creating a WS-Notification implementation within CXF. Progress in this was slowed significantly by documentation for the framework being extremely limited. While creating this implementation, it was discovered that an implementation existed, undocumented, within Apache CXF. Through experimentation and knowledge gained from attempting to build a bespoke implementation, a basic demo was produced, subscribing a consumer to a preallocated topic and sending out a message through the subscription.

\subsubsection{Review}

At the end of the first sprint we completed our goals. We had Eclipse with Maven working on everyone's machines and using Github, we shared the code that had been written so that in future sprints everyone would be able to work on the code.

The major issue in this sprint was the bottleneck caused by the coding. It was impossible for us to divide up the programming work between us which made it difficult to progress through the work quickly and to keep everyone informed about the work. Although it was inefficient, by having one team member write the code while everyone else researched, this did mean that everyone was at least familiar with the project so that they would be able to work efficiently in future sprints. At the end of the sprint, the team had meeting without the customer where the code was reviewed so that everyone understood it and could work on it in the future sprints.

There were issues with writing the user stories to Jira. Peter Prince was the only person who could get onto the online service due to issues with the web plugin on other team member's machines. This made the point of assigning work to each other on Jira useless as people couldn't see what work they had been given. This issue was solved in our next sprint where Jira was replaced with github issue tracking.


\subsection{Sprint 2}

\subsubsection{Sprint Goals}

In the second sprint we were set the goals of expanding on the work we had already done. At the end of the first sprint we had a producer, broker and consumer all with hard-coded values for the subscription information. Our target for the end of the second sprint was to have a consumer that subscribed with configurable topics and to have multiple consumers listening to a broker.

\begin{table}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \emph{Origin}         & \emph{Destination}    & \emph{Route}                 & \emph{Consumers} \\ \hline
    One Hard-Coded & None           & None                  & One       \\ \hline
    Configurable   & One Hard-Coded & One Hard-Coded        & Two       \\ \hline
    ~              & Configurable   & Multiple Hard-Coded   & Multiple  \\ \hline
    ~              & ~              & Multiple Configurable & ~         \\ \hline
    \end{tabular}
\caption{Sprint 2 Table}
\label{tab:sprint2}
\end{table}

During the second sprint planning meeting we made a table of requirements with Snowflake. The table split the tasks into layers that had to be implemented row by row, but within each row we could implement the columns in any order. At the start of the sprint we had implemented the top row of the table and the target for the end of the sprint was to implement the lowest row from each column. The process works by filling out each layer of the table, one at a time to make sure features could be implemented in feasible steps.

\userstory{Notification Filtering By Airport}{flight planner}{to be notified if there is a change to airports I'm interested in for my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

\subsubsection{Work Allocation}

Peter and James worked on rewriting the code so that multiple subscriptions were possible and could be configured by command line arguments. Charles was assigned writing tests for the system using Cucumber.

\subsubsection{Implementation}

In practice we found that the jump from hard coded values to having everything being configurable was trivial. However creating tests for the code was not as simple as we had hoped. We created a set of Cucumber tests to check that a NOTAM could be sent from a producer, to a broker and out to a set of consumers; we ran into problems with waiting for processes to load up and having time-outs to check if a NOTAM had been received.

\subsubsection{Review}

It was at this point, after producing a large number of user stories for this sprint, that we made the decision to cease using JIRA as our issue tracker. At this point in the project we were getting bogged down by the many unused pages and tools of JIRA which served no purpose in our small project, so we opted to fully commit to Github's issue tracker as it was already in use in a minimal capacity. While JIRA initially proved useful early on in the project, having to install and use the VPN software on every computer we wished to access the tools on meant the issue tracker provided by our source repository (Github) ended up being temporarily used as a middleman. Shortly after realising that this was not an efficient use of time, we debated leaving JIRA in favour of fully committing to Github. While a number of tools offered by JIRA did not exist in Github's issue tracker, this often worked against JIRA as it contained a large array of tools not useful for a small scale project such as ours which often got in the way. As Github was already being used as our source repository, bringing issue tracking and source control into one place seemed like the most efficient thing to do.

Having tests for the system would have been useful but they only worked on certain team member's machines. This made the tests counterproductive as we didn't know if the system was working or not. There were also issues with setting up and pulling down services, such as the notification broker, between tests. Additionally since the tests ran every time the code was compiled this slowed the development process considerably when we had to include long timeouts for the tests. In sprint 3, the tests were moved to Travis which solved all these problems.

\subsection{Sprint 3}

\subsubsection{Sprint Goals}

The main focus of sprint 3 was to research the extent of JBoss A-MQ's filtering capabilities, to access its viability for use in the project. Through research in the previous sprint, the prominence of JBoss as a message queue used industrially had become apparent. Research done by Snowflake themselves also suggested that it would be useful for the project. The subject of testing was also brought up, and the implementation of a continuous integration system was decided upon to streamline the implementation of changes to the project. Early on in the sprint, Jenkins was implemented and set up to run tests continuously on the latest version of the project, updating the project's Github page with the outcome of the latest tests.

It was in sprint 3, after the progress made up to that point was shown in the sprint meeting, that the customer expressed a desire for the implementation of spatial and temporal filtering of NOTAMs. The utility of spatial filtering is obvious in the case of flight planning, a flight planner simply defining the start and end point of the journey and then receiving all messages relevant to that geographical space would save considerable time when compared to the alternative of manually subscribing to airspaces and airports which are within range. Temporal filtering would allow a consumer to only receive NOTAMs that are in effect for the duration of their flight, dramatically reducing the number of irrelevant NOTAMs being sent.

In order to perform spatial filtering, latitude and longitude data for every airspace and airfield would be required, as NOTAMs lack information at this level of detail. The information required for temporal filtering is featured in each NOTAM, and only conversion to a usable timestamp format was required.

Additionally we wanted to fix the problems we had with the Cucumber tests. The tests were slow and would only work on some of our machines, we made it a goal of the team to find a way to remedy this issue. We decided on moving the tests to a continuous integration server and to not run the tests every time we compiled.

\subsubsection{Work Allocation}

Charles continued working on the Cucumber tests, writing more case scenarios and fixing bugs in the old tests. He also set up the Github repository to work with the the continuous integration service on Travis.James was assigned changing the system over to JBoss. In the middle of the sprint we realised that JBoss wouldn't be able to meet our requirements and so James then started to work on a JMS system. Pete looked into temporal and spatial filtering and was set the task of writing a basic test scenario to check we could filter GPS coordinates.

\subsubsection{Implementation}

With these two filtering types being the goal of the sprint, work was split into exploring documentation for implementation instructions and information regarding filtering, in JBoss A-MQ. After a week of investigation it became apparent that JBoss would not allow us to fulfil the requirements of the product, due to its extremely limited filtering implementation. JBoss filtering is restricted exclusively to simple strings, comparing the contents of each message with a given filter and sending based on its presence. This left two options: either forking A-MQ and modifying it to support the required features or dropping it in favour of a message queue which does fulfil those requirements. Basing the decision on the scope of the project and limited remaining time to complete it, it was decided to abandon A-MQ. As it was early in the sprint and the outcome of the decision would dramatically affect the outcome of the sprint, the customer was notified of the decision, to ensure it was satisfactory to them.

Moving the Cucumber tests to Travis was done with little difficulty. As the team was already using Maven for the project, the only work that was required was in creating a configuration file to call the relevant tests. The issues with startup and teardown with the Cucumber tests was fixed by rewriting the consumer and producer code so that they could be run in a single process rather than in multiple ones. When the tests were moved to running on Travis rather than on a compile, they were instead run every time a commit or a merge request was made which worked well with the Git workflow of the group.

\subsubsection{Review}

We had JMS with basic temporal filtering and travis tests

\subsection{Sprint 4}

The forth sprint meeting consisted of bringing up to speed members of Snowflake in attendance who had not been notified about JBoss A-MQ. As well as the customer representative, Eddie, and the scrum master, Adrian, Alexis Brooker also attended, and after discussing the need for GPS locations in order to perform spatial filtering, Alexis provided access to a HTTP-GET web feature service (WFS), which returns latitude and longitude locations for all European airfields. Ideally, calling this service would be implemented into AES itself, however given the time remaining on the project and the status of the product as a proof of concept, it was elected to simply retrieve data for all airfields and hard-code them into the system for use in spatial queries, knowing that the system could easily be modified to directly call the service and utilise the data if more time was spent on it.

\subsubsection{Sprint Goals}

After JBoss A-MQ proved to be a dead end, a new system structure was decided on, with only the consumer side using WS-Notifications to communicate. All remaining communication between brokers and producers was changed to rely on JMS, with converters between WS-Notification topics and JMS filters becoming a primary focus for the sprint.

We also needed to implement spatial and temporal filtering and required NOTAM enrichment to do this. 


\subsubsection{Work Allocation}

Charles was assigned temporal filtering of NOTAMS.

James had to add WS Notifications standard to the system

Pete did spatial filtering

Andrew created an xml with a subset of airport locations.

\subsubsection{Implementation}

James couldn't get WS notifcation working due to illness

\subsubsection{Review}

We had JMS working but not WS Notification

\subsection{Sprint 5}

\subsubsection{Sprint Goals}
\subsubsection{Work Allocation}
\subsubsection{Implementation}
\subsubsection{Review}

\subsection{Final Meeting}

\section{Conclusions}
\label{sec:conclusions}

\newpage

\section{Future Work}

\newpage

\addcontentsline{toc}{section}{References}
\sloppy
\printbibliography

\end{document}
