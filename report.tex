\documentclass[a4paper, 12pt]{article}
\usepackage[backend=biber]{biblatex}
\usepackage{graphicx}
\usepackage{caption}

%Package stuff for the user stories
\usepackage{framed}
\usepackage{tikz}
\usepackage[a4paper,margin=1cm,landscape]{geometry}
\usetikzlibrary{decorations.pathmorphing,calc,positioning,shapes,shadows,arrows}
\tikzstyle{abstract}=[rectangle, draw=black, rounded corners, fill=gray!80, drop shadow,
        text centered, anchor=north, text=white, text width=3cm]
\tikzstyle{comment}=[rectangle, draw=black, rounded corners, fill=white, drop shadow,
        text centered, anchor=north, text=white, text width=3cm]
\tikzstyle{myarrow}=[->, >=open triangle 90, thick]
\tikzstyle{line}=[-, thick]
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate
     }}

     % Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border]
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white,
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

%End User story package stuff

\addbibresource{report.bib}
\nocite{*}

\newcommand{\userstory}[4]{
	\begin{parchment}[User Story: #1]
	\noindent\emph{\textbf{AS A}} {\small #2}\\*\emph{\textbf{I WANT}} {\small #3}\\*\emph{\textbf{SO THAT}} {\small #4}
	\end{parchment}
}

\begin{document}

\begin{titlepage}

	\center

	{\large ELECTRONICS AND COMPUTER SCIENCE}\\[0.2cm]
	{\large FACULTY OF PHYSICAL SCIENCES AND ENGINEERING}\\[0.2cm]
	{\large UNIVERSITY OF SOUTHAMPTON}\\[3cm]

	{\Large Peter Prince}\\[0.2cm]
	{\Large James Robinson}\\[0.2cm]
	{\Large Charles Sherman}\\[0.2cm]
	{\Large Andrew Sullivan}\\[1cm]
	{\Large \today}\\[3cm]

	{\LARGE Aeronautical Event Service}\\[3cm]

	{\large Project Supervisor: Dr Robert Walters}\\[0.2cm]
	{\large Second Examiner: -}\\[2.5cm]

	{\large A project report submitted for the award of}\\[0.2cm]
	{\Large MEng Computer Science}

\end{titlepage}

\begin{abstract}

\end{abstract}

\newpage

\section*{Acknowledgements}

\newpage

\tableofcontents
\newpage

\listoffigures
\newpage

\section{Introduction}
\label{sec:introduction}

The project customer was Snowflake Software, a company that works in geographical data exchange services. The focus of this project is the exchange of aeronautical data between flight planners and data publishers such as the Federal Notam Service and the Aeronautical Information Service. Snowflake were planning to implement a data exchange service for clients that would allow consumers to recieve aeronautical data in the form of `Notice to Airmen', using a push notification service. Our group was given the task: ``To demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. In practice, our work was to explore what architectures existed to do push notification and which would work with the requested standard of WSN.

\newpage

\section{Project Description}
\subsection{Motivation}
Pilots need to be notified of hazards and events that may require them to make changes to their flight plan. Examples of such events includes: closed runways, restricted airspaces, extreme weather conditions and even the presence of migrating birds through an airspace. Currently, flight planners receive such information by pulling NOTAM's, or `Notice to Airmen', and must manually sort through all NOTAMs to find only the notices which are relevant to their flight, which is extremely time consuming. The efficiency of this task could be improved dramatically if the filtering were performed automatically using defined geographic regions and time frames. The aeronautics industry uses WSN as their prefered standard, so our customer required us to work to this standard or WS-Events.

\subsection{Customer}

The customer for this project was Snowflake Software, a company focused on open data exchange, and various products based on mapping data. Within sprint meetings the company was represented by Eddie Curtis, who expressed the feature requests and gave feedback, representing Snowflake as a whole.

\subsection{Goals}

The vision, as stated by Snowflake, was ``To develop a proof of concept push event service for delivering digital NOTAM updates to pilots; to demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. From this vision, the goal of the software produced over the course of the project was to explore options and find methods to limit the incoming data to a flight planner to only relevant data by utilising a publish and subscribe model, removing the need for manual filtering and the pulling of relevant NOTAMs by pilots.

The subscriptions to this service had to be based on the relevancy of NOTAMs to the recipients. This relevancy was determined by geographic data, such as departure and arrival airports, and more complex temporal queries, for example, limiting NOTAMs events to ones relevant only during the course of the flight. The software was required to use open standards for the publish and subscribe architecture, for example: OASIS Web Services Notification or WSN. The software also had to be capable of handling an estimated 10,000 messages per day. These messages come from two sources: the Federal NOTAM Service, which publishes a baseline NOTAM every 28 days, and updated delta files which arrive every 15 minutes.

\subsubsection{Project Goals}
\begin{enumerate}
\item Capable of parsing XML documents from the AIXM schema.
\item Capable of basic filtering of NOTAMs based on simple text fields such as airport names.
\item Capable of temporal filtering of NOTAMs.
\item Capable of receiving and handling subscriptions from consumers.
\item Capable of publishing parsed NOTAMs to relevant subscribers.
\item Produced utilising exclusively open-source libraries.
\item Using WS-Notifications to securely communicate all NOTAMs.
\item Able to handle 10,000 messages per day.
\item Store and update baselines to provide to consumers upon connection.
\end{enumerate}

\subsubsection{Additional Goals}
\begin{enumerate}
\item Spatial filtering of NOTAMs based on distances from points.
\item Spatial filtering of NOTAMs based on locations within bounding boxes.
\item Retrieve GPS co-ordinates for airports and airspaces from Snowflake's Global Aerodromes web service.
\item Enriching NOTAMs with GPS co-ordinates to allow for faster spatial filtering.
\item Handle cases where the Global Aerodromes service does not contain a designator for a location.
\end{enumerate}

\subsubsection{Project Scope Changes}
In the opening meeting, the initial project goals were set out by our liason with Snowflake, Adrian. Several meetings later this list of requirements was updated, making a spatial filtering a major focus for the project and temporal filtering an extension goal to complete if we had time. As most of the groundwork for both filtering types could be reused, both were completed.

The storing and provision of baselines ceased to be a requirement after a few meetings, when it was revealed to us that Snowflake already possessed a service which provided updated NOTAM baselines and this functionality was not required of any system we produced.

\section{Background Information}

\subsection{NOTAMs}
A `notice to airmen' (NOTAM), is a notice given to airmen about events in a particular airspace or at a particular airport. The events may be hazards that have to be avoided, such as dangerous weather or updates on runway closures, which may or may not be relevant to the flight. They're used by pilots and flight planners to notify them about events which have the potential to affect the safety of their flight. NOTAMs are distributed by various aviation authorities around the world and are generally received by pilots and flight planners by pull notifications.

The NOTAMs are delivered as deltas from a baseline of data. For instance, a pilot may be told in a NOTAM that runway 3 has been closed at their destination. They were previously told about the number of alternative runways at the destination airport in a baseline message. Deltas can be either temporary or non-temporary; temporary updates apply for a certain time frame whereas non-temporary updates last indefinitely. If there are many delta updates, it is often useful to make a snapshot. A snapshot is an updated baseline that is formed from the original baseline and a combination of deltas, this allows flight planners requesting a NOTAM after a large number of delta updates to quickly receive up to date information without having to request large numbers of delta updates.

All NOTAMs are sent as XML documents using a schema known as AIXM. The AIXM, or \emph{Aeronautical Information Exchange Model}, is a specification for aeronautical information flows and includes two major components: the AIXM conceptual model and its corresponding XML schema. The conceptual model is used to model all important properties, features and restrictions which govern all aeronautical information. Features include runways, aerodromes and airspaces, whereas properties can include information such as the width and length of a specific runway. This model is implemented as the AIXM XML schema which all NOTAMs are created using.

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth}
\includegraphics[scale=0.5]{aixmUML.png}
\end{center}
\caption{UML diagram highlighting some features of the AIXM conceptual model.\cite{aixm}}
\label{fig:aixm}
\end{figure}

\newpage

\subsection{Publish/Subscribe Architecture}

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth}
\input{broker.pdf_tex}
\end{center}
\caption{Publish/Subscribe Architecture with Broker}
\label{fig:pubsub}
\end{figure}

Publish-subscribe is a messaging architecture that allows decoupling between the \emph{producers} and the \emph{consumers} of \emph{messages}. Messages are categorised into \emph{topics}; producers publish messages on a given topic with no knowledge of which (if any) subscribers the topic has, and consumers subscribe to a given topic with no knowledge of which (if any) producers are publishing messages with that topic.

This architecture is generally implemented using a \emph{broker}, which exists as middleware between the producers and consumers. Producers send messages on a given topic to the broker, which then decides which consumers the messages should be forwarded to. Consumers subscribe via the broker to topics they’re interested in, and the broker forwards relevant messages to them. This architecture has a number of benefits, including providing loose coupling between producers and consumers (since they need no knowledge of each other to make use of this architecture) and facilitating scalability by supporting arbitrary numbers of producers and consumers for any given topic.

In Java, brokers typically implement the Java Message Service (JMS) API provided by Java Enterprise Edition, which offers a consistent interface for communicating with many different message broker implementations.

\subsection{Web Services Notification}

Web services notification (or WSN) is a set of specifications created by OASIS that defines the interactions between web services, using notifications, which can form the basis of a publish/subscribe architecture built using web services. They provide a standardized way to communicate with and between web services, without having prior information about them. It includes facilities for detailed subscription, where those wishing to consume information can register with a web service along with details about the type of information they wish to receive. WSN was designed to revolve around these subscription details known as ``topics'' and use them to filter down subscriptions, allowing for large scale usage of the publish/subscribe architecture without unwanted message overload.

WSN covers a family of smaller specifications including WS-Base Notifications, WS-Brokered Notification and WS-Topics. WS-Base Notifications is a specification of the simple producer/consumer relationship, consumers send out subscription requests to the producers, specifying the subset of messages produced by that producer which it wishes to receive. The request also contains a reference to the consumer itself, so the producer knows where to send the messages.

WS-Brokered Notification works in a similar manner to WS-Base, but introduces a middleman between the message publishers and the message subscribers known as the broker. The broker is designed to decouple the connection between producers and consumers and allows for advanced messaging features such as demand-based publishing, load-balancing and publication of messages from entities which are not themselves web service providers.

WS-Topics is a specification which defines a method for categorising subscriptions into ``topics''. This specification is designed to work in conjunction with WS-Base or WS-Brokered and contains various methods for defining groups of subscriptions.

\subsection{Web Services Description Language}

The Web Services Description Language (or WSDL) is an XML based language for describing the interfaces and functionality of a web service. A WSDL file provides a machine readable description of a web service's interface, including the function calls, parameters they expect and return values.\\
Typically, it is used in combination with SOAP to provide a web service, where a client program will read the WSDL, to determine the functions available, and then use SOAP to call them.

\subsection{JAXB and DOM Parsing}

JAXB, or ``Java Architecture for XML Binding'', is a tool used to marshal Java objects into XML and unmarshal XML into Java objects. It can be used as an alternative to standard XML parsers, taking complicated schema and converting XML created from them into usable Java objects. Use of JAXB was explored towards the end of the project for use in spatial filtering. We intended to use JAXB to parse XML filters of the OpenGIS standard in order to create filters within our system, however implementing JAXB proved beyond the scope of the project in terms of time, given the nature of the system as a proof of concept piece with a strict time constraint. Instead, we settled for a more commonly used XML parsing method known as ``Document Object Model'' or DOM parsing.

DOM parsing is one of the two most commonly used XML parsing methods, the other being SAX parsing. A DOM parser works by loading the entirety of an XML document into memory, modelling it as a tree to allow it to be easily traversed. The tree is traversed node by node to obtain the required information, in this case known tags within the OpenGIS standard for use in spatial filtering. This method of parsing generally has quite a large overhead when parsing large XML files, due to the fact it needs to parse the entire file.

\subsection{XML Catalogues}

XML files contain references to external XML resources such as schema files and WSDL files. When referencing a large number of external files, events such as network outages and slow connections can cause major issues for a service and its performance. Another problem which can occur when referencing external resources is relative URLs being used to address them. Relative URLs limit the service using them to the context in which it was created, as launching it from any other location will not work.

A solution to these problems is creating an XML catalogue, which allows you to store these files locally and instruct your service to use these local references when its external counterpart is requested. The catalogue maps between these two versions of the same resource.

\subsection{JIRA Issue Tracking}

Issue trackers are systems which maintains lists of issues in an ongoing project, with the ability to assign these issues to specific group members, report bugs while working and connect solved issues to source control commits which fix them. Issues can also be broken into sub-tasks, prioritised and categorised to improve efficiency. For example, many issues were recorded after each sprint meeting, these included work for the next sprint, work for near future sprints and overall project goals. These issue reports were categorised according to the sprint which they were to be completed in. When every issue attached to a sprint was marked as solved, a sprint was considered complete in terms of work.

As our customer is a company which regularly works on projects of a much greater scale than that of our AES system, they possess a number of professional tools for organisation and issue tracking. One of these is JIRA, an issue tracker created by Atlassian designed to work with our chosen cycle of creating user stories and acceptance criteria with the customer and assigning these stories to team members after each sprint meeting. As JIRA is a professional piece of software which normally requires a monthly fee for usage, we had to gain access to the server running it licensed to our customer through their VPN. By using the tools in use by our customer, they were able to easily keep track of progress and make comments on submissions to the service.

\subsection{ActiveMQ}

Apache ActiveMQ is an open source message queue service which communicates using the Java Message Service (JMS). It contains a number of features deemed useful to the project including subscription management and filtering. The filtering itself is through objects known as selectors, which use the SQL 92 syntax to filter messages. SQL 92 is a fairly barebones SQL syntax, which features basic arithmetic operators (addition, subtraction, multiplication, division and modulo), enough for the requirements set out by the customer of temporal filtering and spatial filtering using GPS co-ordinates.

\subsection{Git and GitHub}

Git is a distributed version control system, which allows multiple people to work on a single project at the same time, without necessitating the use of a shared network, as well as tracking document history, affording reversions or rollbacks to previous versions. Some form of source control is essential for effective collaboration between programmers and greatly aides coordination and communication.

GitHub is a Git repository hosting service, which also offers other features that aide collaborative coding, such as issue tracking and task assignment.

\subsection{Travis CI}

Travis CI is a free, hosted, continuous integration platform that is used to automatically build and test projects hosted on GitHub. It automatically detects when a commit has been pushed to a linked GitHub repository and attempts to build all branches and run tests. It can then be set up to show the results of this in the GitHub readme or to notify the developer in some other way, such as by emailing the results.

This practice of continues integration allows developers to detect problems quickly. This also makes fixing them easier, as less back tracking is required to identify when and where the issue occurred, reducing the number of large scale revisions required. It also means they can assure the software will run on a variety of computer set-ups, without time consuming manual testing.

\subsection{Cucumber}

Cucumber is a testing suite that runs behaviour-driven development style acceptance tests, i.e., tests written to check if the requirements of a specification are met by the code. These focus on testing behaviours, i.e. the action the code is supposed to perform, where each test goal is independent of the code’s structure, rather than an approach like unit testing, that tests individual units of code.
A single Cucumber test consists of a plain text feature definition, written in plain English, and corresponding step definitions, that is, the code that actually tests the feature.

A feature definition describes a single behaviour the code is supposed to be able to perform (such as ``Feature: Filter NOTAMs by time'') and then outlines one or many scenarios: sets of conditions that test a facet of the behaviour. These describe initial conditions, steps that are then carried out, and what the results of performing those steps (with those initial conditions) should be. These are specified with the keywords ``Given, When, Then'', i.e., given a set of initials conditions, when something happens, then the program should do something (with the ability to have multiple steps with the keyword And).

For each feature, there is then a set of corresponding step definitions, that is, code to test the feature, in the form of a subroutine for each step (e.g. there will be a single subroutine for ``Given a broker is running'', which will check that a broker is running and, if not, launch one). Depending on the keyword, these should either set up the initial conditions (in the case of Given or a following And), perform a particular action (in the case of When or a following And), or assert that particular conditions are true (in the case of Then or a following And). If all conditions are satisfied, the test passes, otherwise it fails.

\subsection{JSON}

JSON (or JavaScript Object Notation) is a human readable data interchange format and an alternative to XML. It transmits data objects consisting of attribute-value pairs and is primarily used for server-web application communication. One of its primary advantages is its relative concision and lack of redundancy when compared to XML.

\subsection{SOAP}

SOAP (or, originally, Simple Object Access Protocol) is a platform independent protocol for exchanging information between web services, based on XML, and is dependant on other application layer protocols for message transmission (typically HTTP but other transport protocols are supported as well).

\subsection{Netcat}

Netcat is a utility to read from and write across network connections using TCP or UDP. It is designed so that it can easily and reliably be used as a back-end tool, so it can be used by other programs. It is frequently referred to as the `Swiss army knife' of networking tool because of its wide array of features, such as port scanning and a large number of network debugging and investigation tools.

\subsection{cURL}

cURL is both a library and command line tool for sending and retrieving data using a wide variety of protocols (such as HTTP, FTP, SCP, POP3, etc.). It is very portable, working on a wide variety of platforms, and bindings are available for a large number of programming languages.

\subsection{Web Feature Service}

\newpage

\section{Planning and Organisation}
\label{sec:planning}

The project was built using the Agile software development practices of Scrum, and each sprint followed a similar structure. Every other week, our team would meet in  First, the customer came up with ideas for features which could be worked on in the following sprint. Next, user stories based on these features were created, dividing up the requirements of each feature and helping to create acceptance criteria which would come together to prove acceptance that the feature was successfully implemented.

At the beginning of each sprint, a sprint meeting was held to both close the previous sprint and prepare for the following one. First, features implemented in the previous sprint were demonstrated to the customer, allowing them to track the progress of the project and decide whether the feature had been successfully implemented. In the event the customer was not satisfied with the work done in the sprint, the user stories were kept in the backlog for the following sprint. Otherwise, the customer discussed future features to be implemented and broke the feature up into user stories to be added to the backlog.

Meetings also regularly featured exercises to identify issues and find solutions to problems faced by the group over the previous sprint. Before moving onto work that needed to be done for the following sprint, each group member and the customer filled in a table of three columns on a whiteboard. These columns listed points we felt happy about in terms of project progression, points we were unsure about and points we felt required discussion. This list was then prioritised and discussed in order, finding solutions for problems such as group dynamic issues and finding ways to continue with things we agreed were going well. These exercises not only created a more open forum for members of the group to start discussions about things they would otherwise be uncomfortable starting, but allowed the customer to be made aware of aspects of the project which were working well and we were proud of.

Various software was used to carry out this process, including JIRA (access to which was provided by the client) and Cucumber. JIRA is a tracker that manages user stories and their corresponding acceptance criteria as per each feature. User stories within JIRA are stored in the standardised \emph{``As a, I want, So that''} format, with their acceptance criteria written as \emph{``Given, When, Then''}. This format is also used by Cucumber for acceptance tests. Team members on JIRA can choose what user stories to complete or they can be assigned.

Early on in the project we were given access to Jira on Snowflake's private servers. This allowed us to keep control over our backlog and keep a record of what had been completed. However over the course of the project, the number of steps required to connect to the Snowflake VPN and access Jira as well as the difficulty from never having used the service before, we decided keeping track of the project would be better done through Github, which was already being used as a code repository. The issue tracking features within Github replaced Jira's extensive features (the majority of which far surpassed the requirements of a small project such as ours) and all tasks became issues, still using the standard user story format.

\subsection{Skills Audit}
To maximise the efficiency of the team and to minimise the amount of time which needed to be spent learning new skills for the project it was necessary to assign tasks according to relevant skills and experience. In order to do this it was necessary to produce a skills audit (shown below) and divide tasks according to it.

\begin{table}[h]
\begin{tabular}{|l|l|}
\hline
\emph{Team Member} & \emph{Relevant Skills}                           \\ \hline
Andrew Sullivan    & Java, Python, JUnit                              \\ \hline
Charles Sherman    & Java, WSDLs, SOAP-UI, JUnit, Cucumber, SCRUM     \\ \hline
Peter Prince       & Java, Python, JUnit                              \\ \hline
James Robinson     & Java, Maven, Git, JIRA, Travis, Cucumber, JUnit  \\ \hline
\end{tabular}
\end{table}

During each sprint meeting the work for the following sprint was decided upon, we then met in labs after the meeting to break down the sprint work further and assign each key area to a team member according to the relevant skills. Due to the nature of the project, a lot of early work on the project was not easily divisible, so a system of a single team member coding as the rest of the team researched and provided support.

\subsection{Risk Analysis}

\subsection{Customer Relations}

Our customer was Snowflake Software, a Southampton-based company specialising in open data exchange solutions. Over the course of the project we had regular sprint meetings held with a number of Snowflake employees including one of the CEOs: Ian Painter, the chief technology officer: Eddie Curtis, the professional services director: Alex Brooker, head of software development: Adrian McKenzie (who acted as our main liason with the company) as well as a number of members of their development team who were present in the final presentation to the company of the finished product. Communication with Adrian was done through both emails and the fortnightly Scrum meetings.

\subsection{Deliverables}

\subsection{Git workflow}

\subsection{Testing Methodology}

Behavioural-driven development style acceptance testing was used, using Cucumber to express the acceptance criteria for each story, allowing them to be converted into automated acceptance tests which could be run using JUnit. The scenarios were written such that the implementation of the feature can change without having to rewrite the file. For each story, when all acceptance tests passed within JUnit, the user story is removed from the backlog. This type of testing keeps testing focused on user stories, testing features rather than abstract units of code, better ensuring that the code satisfies the client's requirements, rather than some abstract measure of performance.

Continuous integration was also used, utilizing Travis CI. This helps to ensure new code doesn't break old tests, ensures the software can run on a variety of computer set-ups, and allows problems to be detected more quickly and fixed with less backtracking.


\newpage

\section{Implementation}

Due to the exploratory nature of the project, there was a lot of work that went into the project that wasn't present in the end result. The team worked using SCRUM and completed five sprints. In our implementation, we discuss each sprint individually to show the alternatives we explored and to show the work that didn't make it to the final prototype.

\subsection{Sprint 1}

At the start of the project the team had a meeting with Snowflake, we were introduced to the project and we discussed the scope, requirements and overall goals of the project. Snowflake described the vision of the project, the goal we were set was: ``To demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. This also had to be implemented using either the WS-Notification standard or WS-Events, as these are the most used standards in the industry.

\subsubsection{Sprint Goals}

For the first sprint the goal was to have a basic push notification system running that could publish and receive an XML. As well as this we had to set up our development environment and tools for the project.

The project was written in Java because of the familiarity the team already had with it. The team worked in the IDE Eclipse, and used the build system Maven to handle library dependencies as it has automatic dependency resolution and plugins for code generation. For version control the team used Github.

For the first sprint there was only a single user story to implement. Jira\ref{JIRA} was used to manage our user stories and to allocate the work, however there were problems using it.

\userstory{Basic Push Notification}{flight planner}{to be notified if there is a change to my destination airport for my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

To implement the user story required setting up Apache ActiveMQ\ref{activeMQ} and implementing a method of providing a message to the broker and a way of displaying the message so tests could be written to see that it was recieved correctly.

\subsubsection{Work Allocation}

As the sprint goal was very simple it was difficult to separate the work between team members. James did the majority of the coding work while everyone else in the group assisted by researching the possible tools we could use. This was in effect an extension of pair-programming, where two people working on a single machine take turns in programming while the other checks for mistakes.

Once we had found a solution that worked, James presented the code to everyone else so that the group understood which of the implementations we went with and why.

Apart from programming, James also helped to set up Maven on the team member's machines so that they could also work on the code. Charles Sherman set up the github repository to store the code for the project and Peter Prince set up the team's project on Jira.

\subsubsection{Implementation}

After working on a basic implementation of ActiveMQ and exploring the documentation available online, it became obvious that it contained no implementation of WS-Notification and the page within its online documentation which was supposed to detail its implementation was a dead link. One of the requirements stated by the customer was the inclusion of WS-Notification and while this was a soft requirement, meaning that it's inclusion could be forgone if sufficient justification was given. The project was in an early enough stage at this point in development that it was worth exploring other alternatives. Alternatives such as developing our own WS-Notification implementation using CXF.

Apache CXF is an open source framework for producing web services, and after ActiveMQ was deemed unusable, research began into using CXF. Work started by creating a WS-Notification implementation within CXF. Progress in this was slowed significantly by documentation for the framework being extremely limited. While creating this implementation, it was discovered that an implementation existed, undocumented, within Apache CXF. Through experimentation and knowledge gained from attempting to build a bespoke implementation, a basic demo was produced, subscribing a consumer to a preallocated topic and sending out a message through the subscription.

\subsubsection{Review}

At the end of the first sprint we completed our first user story and set up the development environment for all the team members. We had Eclipse with Maven working on everyone's machines and using Github, we shared the code that had been written so that in future sprints everyone would be able to work on the code.

The major issue in this sprint was the bottleneck caused by the coding. It was impossible for us to divide up the programming work between us which made it difficult to progress through the work quickly and to keep everyone informed about the work. Although it was inefficient, by having one team member write the code while everyone else researched, this did mean that everyone was at least familiar with the project so that they would be able to work efficiently in future sprints. At the end of the sprint, the team had meeting without the customer where the code was reviewed so that everyone understood it and could work on it in the future sprints.

There were issues with writing the user stories to Jira. Peter was the only person who could get onto the online service due to issues with the web plugin on other team member's machines. This made the point of assigning work to each other on Jira useless as people couldn't see what work they had been given. This issue was solved in our next sprint where Jira was replaced with github issue tracking.

\subsection{Sprint 2}

\subsubsection{Sprint Goals}

In the second sprint we were set the goals of expanding on the work we had already done. At the end of the first sprint we had a producer, broker and consumer all with hard-coded values for the subscription information. Our target for the end of the second sprint was to have a consumer that subscribed with configurable topics and to have multiple consumers listening to a broker. It was also important to develop tests to demonstrate the the customer that the software was working, and for the team to manage bugs that arose in development.

\begin{table}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \emph{Origin}         & \emph{Destination}    & \emph{Route}                 & \emph{Consumers} \\ \hline
    One Hard-Coded & None           & None                  & One       \\ \hline
    Configurable   & One Hard-Coded & One Hard-Coded        & Two       \\ \hline
    ~              & Configurable   & Multiple Hard-Coded   & Multiple  \\ \hline
    ~              & ~              & Multiple Configurable & ~         \\ \hline
    \end{tabular}
\caption{Sprint 2 Table}
\label{tab:sprint2}
\end{table}

During the second sprint planning meeting we made a table of requirements with Snowflake. The table split the tasks into layers that had to be implemented row by row, but within each row we could implement the columns in any order. At the start of the sprint we had implemented the top row of the table and the target for the end of the sprint was to implement the lowest row from each column. The process works by filling out each layer of the table, one at a time to make sure features could be implemented in feasible steps.

\userstory{Notification Filtering By Airport}{flight planner}{to be notified if there is a change to airports I'm interested in for my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

\subsubsection{Work Allocation}

Peter and James worked on rewriting the code so that multiple subscriptions were possible and could be configured by command line arguments. Charles was assigned writing tests for the system using Cucumber.

\subsubsection{Implementation}

In practice we found that the jump from hard coded values to having everything being configurable was trivial. However creating tests for the code was not as simple as we had hoped. We created a set of Cucumber tests to check that a NOTAM could be sent from a producer, to a broker and out to a set of consumers. There were issues with starting multiple processes up and having time-outs to check if a NOTAM had been received.

\subsubsection{Review}

It was at this point, after producing a large number of user stories for this sprint, that we made the decision to cease using JIRA as our issue tracker. At this point in the project we were getting bogged down by the many unused pages and tools of JIRA which served no purpose in our small project, so we opted to fully commit to Github's issue tracker as it was already in use in a minimal capacity. While JIRA initially proved useful early on in the project, having to install and use the VPN software on every computer we wished to access the tools on meant the issue tracker provided by our source repository (Github) ended up being temporarily used as a middleman. Shortly after realising that this was not an efficient use of time, we debated leaving JIRA in favour of fully committing to Github. While a number of tools offered by JIRA did not exist in Github's issue tracker, this often worked against JIRA as it contained a large array of tools not useful for a small scale project such as ours which often got in the way. As Github was already being used as our source repository, bringing issue tracking and source control into one place seemed like the most efficient thing to do.

Having tests for the system would have been useful but they only worked on certain team member's machines. This made the tests counterproductive as we didn't know if the system was working or not. There were also issues with setting up and pulling down services, such as the notification broker, between tests. Additionally since the tests ran every time the code was compiled this slowed the development process considerably when we had to include long timeouts for the tests. In sprint 3, the tests were moved to Travis which solved all these problems.

\subsection{Sprint 3}

\subsubsection{Sprint Goals}

The main focus of sprint 3 was to research the extent of JBoss A-MQ's filtering capabilities, to access its viability for use in the project. Through research in the previous sprint, the prominence of JBoss as a message queue used industrially had become apparent. Research done by Snowflake themselves also suggested that it would be useful for the project. The subject of testing was also brought up, and the implementation of a continuous integration system was decided upon to streamline the implementation of changes to the project. Early on in the sprint, Jenkins was implemented and set up to run tests continuously on the latest version of the project, updating the project's Github page with the outcome of the latest tests.

It was in sprint 3, after the progress made up to that point was shown in the sprint meeting, that the customer expressed a desire for the implementation of spatial and temporal filtering of NOTAMs. The utility of spatial filtering is obvious in the case of flight planning, a flight planner simply defining the start and end point of the journey and then receiving all messages relevant to that geographical space would save considerable time when compared to the alternative of manually subscribing to airspaces and airports which are within range. Temporal filtering would allow a consumer to only receive NOTAMs that are in effect for the duration of their flight, dramatically reducing the number of irrelevant NOTAMs being sent.

In order to perform spatial filtering, latitude and longitude data for every airspace and airfield would be required, as NOTAMs lack information at this level of detail. The information required for temporal filtering is featured in each NOTAM, and only conversion to a usable timestamp format was required.

Additionally we wanted to fix the problems we had with the Cucumber tests. The tests were slow and would only work on some of our machines, we made it a goal of the team to find a way to remedy this issue. We decided on moving the tests to a continuous integration server and to not run the tests every time we compiled.

\subsubsection{Work Allocation}

Charles continued working on the Cucumber tests, writing more case scenarios and fixing bugs in the old tests. He also set up the Github repository to work with the the continuous integration service on Travis.James was assigned changing the system over to JBoss. In the middle of the sprint we realised that JBoss wouldn't be able to meet our requirements and so James then started to work on a JMS system. Pete looked into temporal and spatial filtering and was set the task of writing a basic test scenario to check we could filter GPS coordinates.

\subsubsection{Implementation}

With these two filtering types being the goal of the sprint, work was split into exploring documentation for implementation instructions and information regarding filtering, in JBoss A-MQ. After a week of investigation it became apparent that JBoss would not allow us to fulfil the requirements of the product, due to its extremely limited filtering implementation. JBoss filtering is restricted exclusively to simple strings, comparing the contents of each message with a given filter and sending based on its presence. This left two options: either forking A-MQ and modifying it to support the required features or dropping it in favour of a message queue which does fulfil those requirements. Basing the decision on the scope of the project and limited remaining time to complete it, it was decided to abandon A-MQ. As it was early in the sprint and the outcome of the decision would dramatically affect the outcome of the sprint, the customer was notified of the decision, to ensure it was satisfactory to them.

Moving the Cucumber tests to Travis was done with little difficulty. As the team was already using Maven for the project, the only work that was required was in creating a configuration file to call the relevant tests. The issues with startup and teardown with the Cucumber tests was fixed by rewriting the consumer and producer code so that they could be run in a single process rather than in multiple ones. When the tests were moved to running on Travis rather than on a compile, they were instead run every time a commit or a merge request was made which worked well with the Git workflow of the group.

\subsubsection{Review}

We had JMS with basic temporal filtering and Travis tests

\subsection{Sprint 4}

The forth sprint meeting consisted of bringing up to speed members of Snowflake in attendance who had not been notified about JBoss A-MQ. As well as the customer representative, Eddie, and the scrum master, Adrian, Alexis Brooker also attended, and after discussing the need for GPS locations in order to perform spatial filtering, Alexis provided access to a HTTP-GET web feature service (WFS), which returns latitude and longitude locations for all European airfields. Ideally, calling this service would be implemented into AES itself, however given the time remaining on the project and the status of the product as a proof of concept, it was elected to simply retrieve data for all airfields and hard-code them into the system for use in spatial queries, knowing that the system could easily be modified to directly call the service and utilise the data if more time was spent on it.

\subsubsection{Sprint Goals}

After JBoss A-MQ proved to be a dead end, a new system structure was decided on, with only the consumer side using WS-Notifications to communicate. All remaining communication between brokers and producers was changed to rely on JMS, with converters between WS-Notification topics and JMS filters becoming a primary focus for the sprint.

We also needed to implement spatial and temporal filtering and required NOTAM enrichment to do this.


\subsubsection{Work Allocation}

For this sprint Charles was assigned to the task of building temporal filtering, in parallel to this, Peter worked on spatial filtering, James was tasking with implementing the WS-Notifications standard into the system and Andrew created an XML document which contained a subset of all airport locations, narrowing them to just European aerodromes. Due to the large number of global aerodromes, it was necessary to reduce this just to relevant locations in the interest of parsing time.

\subsubsection{Implementation}

Temporal filtering was implemented by parsing the NOTAM XML and converting the timestamps in the data to a Unix time stamp format. The time stamp measures the time in milliseconds so it is possible to directly compare the times using inequality operators. The code allows a consumer to subscribe to topics after a given time, or within a time period.

The selectors used for filtering are based on SQL 92, which features only simple arithmetic operators. In order to create a proper bounding box which covered a plane route, more operators were required (such as square roots, which are not included). As a result, spatial filtering's implementation was simplified to an axis-aligned bounding box. This type of spatial filtering required simply greater than and less than operators to perform.

To produce the subset XML document, a bounding box was placed over Europe and submitted to the Global Aerodromes service, all aerodromes within the given bounding box were retrieved.

\subsubsection{Review}

All required features of JMS (selectors and subscriptions) were successfully added, however we could not create a functioning implementation of the WS-Notification standard. This goal was moved to the final sprint as a primary focus.

\subsection{Sprint 5}

\subsubsection{Sprint Goals}

The goal for the final sprint was to produce a complete minimal viable product for the product owner. This involved finishing our WS-Notification implementation, packaging the application as a WAR for deployment on Tomcat, building a WSDL for the web service, making a barebones client to demonstrate the service's functionality, and implementing basic OpenGIS filtering support. We also planned to replace our DOM parsers with JAXB in order to make it easier to implement the OpenGIS filtering support.

\subsubsection{Work Allocation}

\begin{tabular}{l|l}
  \emph{Task} & \emph{Assignee(s)} \\ \hline
  JAX-WS code generation using CXF Maven plugin & James \\
  WSDL for our WS-Notification implementation & James, Charles \\
  WAR generation using Maven & Charles \\
  Tomcat setup and WAR deployment & Charles \\
  Mapping from OpenGIS filters to JMS selectors & Peter \\
  Mock client implementation using NetCat and cURL & Peter \\
  Barebones client implementation using JAX-WS & James \\
  NOTAM enrichment & Peter, Andrew \\
  Parsing the Snowflake GlobalAerodromes WFS & Andrew \\
\end{tabular}

\subsubsection{Implementation}

We used the Apache CXF JAX-WS codegen plugin for Maven to build class stubs from the WSN WSDLs. Unfortunately the WSDLs provided by Oasis do not specify any endpoints, and so we created our own based on a subset of the WSN WSDL included with the Apache CXF implementation of WSN.

A mock client for the system was implemented using NetCat and cURL - NetCat took the place of the WSN consumer, listening on port 80 and simply printing out all received data to standard output, and cURL was used to send a subscription request to the WSN broker. Unfortunately we were unable to determine the correct format for the subscription request, so we ditched the barebones client implementation and created a new implementation using CXF, based on the WSDL for our web service.

We originally planned to switch to JAXB for our XML parsing needs during this sprint, but there were so many naming conflicts in the generated code for the XML schemata that it was easier to just manually write DOM parsers instead. After consulting with Snowflake we discovered that they too attempted to use JAXB but abandoned it due to the same issue.

\subsubsection{Review}

We were able to complete all of our goals for this sprint, and produced two projects - the AES service itself, consisting of our producer and WS-Notification broker implementation, and a separate project containing an example consumer.

\subsection{Final Meeting}

\section{Final System Overview}

\begin{center}
\begin{tikzpicture}[node distance=2cm]
     \node (Producer1) [abstract, rectangle split, rectangle split parts=2]
        {
            \textbf{Producer}
            \nodepart{second}JMS Publisher
        };

    \node (ActiveMQ) [abstract, rectangle split, rectangle split parts=1, right=5cm of Producer1]
        {
            \textbf{ActiveMQ}
            
        };

    \node (Producer2) [abstract, rectangle split, rectangle split parts=2, right=5cm of ActiveMQ]
    {
        \textbf{Producer}
        \nodepart{second}JMS Publisher
    };

    \node (WSN Broker1) [abstract, rectangle split, rectangle split parts=1, below left=0.8cm of ActiveMQ]
        {
            \textbf{WSN Broker}
            
        };

    \node (WSN Broker2) [abstract, rectangle split, rectangle split parts=1, below right=0.8cm of ActiveMQ]
        {
            \textbf{WSN Broker}
            
        };

    \node (Consumer) [abstract, rectangle split, rectangle split parts=2, below right=0.8cm of WSN Broker1]
    {
        \textbf{Consumer}
        \nodepart{second}WSN Consumer
    };

    \node (Consumer1) [abstract, rectangle split, rectangle split parts=2, below left=0.8cm of WSN Broker1]
    {
        \textbf{Consumer}
        \nodepart{second}WSN Consumer
    };

    \node (Consumer2) [abstract, rectangle split, rectangle split parts=2, below=1cm of WSN Broker2]
    {
        \textbf{Consumer}
        \nodepart{second}WSN Consumer
    };

    \draw[-latex] (Producer1.east) -- (ActiveMQ.west) node[midway,above] {JMS Message Publish};
    \draw[-latex] (Producer2.west) -- (ActiveMQ.east) node[midway,above] {JMS Message Publish};
    \draw[-latex] (WSN Broker1.north) -- (ActiveMQ.south) node[midway,left] {JMS Subscription};
    \draw[-latex] (WSN Broker2.north) -- (ActiveMQ.south) node[midway,right] {JMS Subscription};
    \draw[-latex] (Consumer.north) -- (WSN Broker1.south) node[midway,right] {};
    \draw[-latex] (Consumer1.north) -- (WSN Broker1.south) node[midway,right] {};
    \draw[-latex] (Consumer2.north) -- (WSN Broker2.south) node[midway,right] {WSN(SOAP over HTTP)};

\end{tikzpicture}
\end{center}

The completed system consists of two parts: a producer, which parses NOTAM data and publishes it to a JMS broker (ActiveMQ, in our project), and a basic WS-BrokeredNotification implementation that allows consumers to retrieve messages from this JMS broker via the WS-Notification protocol. An overview of this system is given in figure X.

The system was designed with scalability in mind, since it needed to be able to handle a large volume of NOTAM traffic, and a potentially large number of consumers. For this reason, an arbitrary number of producers and an arbitrary number of WS-Notification brokers are supported, with each broker servicing multiple consumers. The limiting factor in this setup is the number of simultaneous connections that the JMS broker itself is able to service. Since ActiveMQ already has robust support for clustering, this was deemed to be a non-issue.

Although the consumer was out of this project's scope, we still produced a basic implementation in order to demonstrate the system's functionality. A class diagram for this separate project is given in figure X. Note that this project does not directly reference any of the classes from our main project; instead, it uses the published WSDL for our service, along with JAX-WS, to connect to the service.

\section{Critical Evaluation}

\subsection{Git workflow}

After we switched to the new workflow, this worked pretty well

\subsection{Project management}

Using GitHub issues worked well, but didn't provide us with things like burndown charts, etc. that traditional agile project management tools provide

\section{Conclusions}
\label{sec:conclusions}

\newpage

\section{Future Work}

\subsection{Performance/Scalability Testing}
%Performance/scalability testing (using Amazon EC2 and/or Elastic Beanstalk for example)

\subsection{IATA Designators and ICAO Location Indicators}

Global Aerodromes is a service provided by Snowflake which allowed our system to map from aerodromes referenced in NOTAMs to GPS co-ordinates, a feature required in order to perform spatial filtering. Within the AIXM specification for NOTAMs, both IATA (International Air Transport Assocation) designators and ICAO (International Civil Aviation Organisation) location indicators are specified. Both of these designating IDs are used by Global Aerodromes and the AES service we produced made use of just IATA designators to pair NOTAMs to GPS co-ordinates. This caused an issue due to a fraction of the aerodromes listed by the Global Aerodromes service do not contain a value for this identifier. A possible future extension would be to add a contingency for this occurrence and include support for ICAO location indicators as well, to maximise the chance that every NOTAM's aerodrome is matched to a location for filtering.

\subsection{Compile Time Issues}

A major issue with the final version of the AES system is compile time problems. The reason for these issues is the time required to download all required XML documents and WSDL files. The WSDL files in particular require a large number of external resources including more WSDL files and relevant XSD schemata, each of these referencing even more resources in a long chain. A number of these chained references take a long time to download and as they are referenced multiple times from different locations they are downloaded multiple times, greatly increasing compile times.

A solution to this would be the implementation of XML catalogues which map external references to previously downloaded local files. This would dramatically reduce compile times at the expense of file size, however if the product is to be extended then the time saved would be well worth the cost.

This was not previously implemented due to the team's inexperience at working with them and the compile time creep did not occur until late in the project. At which point a cost-benefit analysis of learning how to use this tool and implementing it into the system at that late a stage was done and decided against.

\newpage

\addcontentsline{toc}{section}{References}
\sloppy
\printbibliography

\appendix

\end{document}
