\documentclass[a4paper, 12pt, twoside]{article}
\usepackage[top=24mm, bottom=24mm, inner=35mm, outer=24mm]{geometry}
\usepackage[backend=biber]{biblatex}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[autostyle]{csquotes}

% Define \fullref command to reference both number and name of section
\newcommand*{\fullref}[1]{\ref{#1} \nameref{#1}}

%Package stuff for the user stories
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{decorations.pathmorphing,calc,positioning,shapes,shadows,arrows}
\tikzstyle{abstract}=[rectangle, draw=black, rounded corners, fill=gray!80, drop shadow,
        text centered, anchor=north, text=white, text width=3cm]
\tikzstyle{comment}=[rectangle, draw=black, rounded corners, fill=white, drop shadow,
        text centered, anchor=north, text=white, text width=3cm]
\tikzstyle{myarrow}=[->, >=open triangle 90, thick]
\tikzstyle{line}=[-, thick]
\pgfmathsetseed{1} % To have predictable results
% Define a background layer, in which the parchment shape is drawn
\pgfdeclarelayer{background}
\pgfsetlayers{background,main}
% define styles for the normal border and the torn border
\tikzset{
  normal border/.style={orange!30!black!10, decorate
     }}

     % Macro to draw the shape behind the text, when it fits completly in the
% page
\def\parchmentframe#1{
\tikz{
  \node[inner sep=2em] (A) {#1};  % Draw the text of the node
  \begin{pgfonlayer}{background}  % Draw the shape behind
  \fill[normal border]
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text will continue in next page
\def\parchmentframetop#1{
\tikz{
  \node[inner sep=2em] (A) {#1};    % Draw the text of the node
  \begin{pgfonlayer}{background}
  \fill[normal border]              % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when the text continues from previous page
\def\parchmentframebottom#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Macro to draw the shape, when both the text continues from previous page
% and it will continue in next page
\def\parchmentframemiddle#1{
\tikz{
  \node[inner sep=2em] (A) {#1};   % Draw the text of the node
  \begin{pgfonlayer}{background}
  \fill[normal border]             % Draw the ``complete shape'' behind
        (A.south east) -- (A.south west) --
        (A.north west) -- (A.north east) -- cycle;
  \end{pgfonlayer}}}

% Define the environment which puts the frame
% In this case, the environment also accepts an argument with an optional
% title (which defaults to ``Example'', which is typeset in a box overlaid
% on the top border
\newenvironment{parchment}[1][Example]{%
  \def\FrameCommand{\parchmentframe}%
  \def\FirstFrameCommand{\parchmentframetop}%
  \def\LastFrameCommand{\parchmentframebottom}%
  \def\MidFrameCommand{\parchmentframemiddle}%
  \vskip\baselineskip
  \MakeFramed {\FrameRestore}
  \noindent\tikz\node[inner sep=1ex, draw=black!20,fill=white,
          anchor=west, overlay] at (0em, 2em) {\sffamily#1};\par}%
{\endMakeFramed}

%End User story package stuff

\addbibresource{report.bib}
\nocite{*}

\newcommand{\userstory}[4]{
	\begin{parchment}[User Story: #1]
	\noindent\emph{\textbf{AS A}} {\small #2}\\*\emph{\textbf{I WANT}} {\small #3}\\*\emph{\textbf{SO THAT}} {\small #4}
	\end{parchment}
}

\begin{document}

\begin{titlepage}

	\center

	{\large University of Southampton}\\[0.2cm]
	{\large Electronics and Computer Science}\\[3cm]

  {\large A Group Design Project report submitted for the award of}\\[0.2cm]
  {\Large MEng Computer Science}\\[3cm]

  {\large by:}\\[0.2cm]
	{\Large Peter Prince}\\[0.2cm]
	{\Large James Robinson}\\[0.2cm]
	{\Large Charles Sherman}\\[0.2cm]
	{\Large Andrew Sullivan}\\[3cm]

  {\large Project supervisor: Dr Robert Walters}\\[0.2cm]
  {\large Second examiner: Professor Mike Wald}\\[3cm]

  {\LARGE Aeronautical Event Service}\\[3cm]
	{\Large \today}\\[3cm]

\end{titlepage}

\begin{abstract}

This group report details the planning, design processes and final implementation of a proof of concept NOTAM push notification system, utilising a publish/subscribe architecture, built for Snowflake Software. The system, named the ``Aeronautical Event Service'' or ``AES'', uses a combination of JMS and WS-Notification communication to handle subscriptions and the publishing of Notices To AirMen. Built in Java, the system parses AIXM XML documents to extract messages which need to be sent to the relevant recipients, storing messages in an ActiveMQ message queue. After receiving subscription requests filtered temporally or spatially, the system is able to send the published messages to the subscribers who requested them.

\end{abstract}

\newpage

\section*{Acknowledgements}
Alex Brooker, Snowflake's professional services director, for providing access to the Global Aerodromes web service. This service was used to obtain GPS co-ordinates for aerodromes and without it spatial filtering and NOTAM enrichment wouldn't have been possible. His advice on the subject of web services was also extremely useful towards the tail end of the project.
\\\\
Adrian McKenzie, Snowflake's head of software development, for providing us with tools and exercises to maximise the usefulness of each sprint meeting, as well as being our liason with Snowflake.
\\\\
Eddie Curtis, Snowflake's chief technology officer, for acting as the product owner and providing with constant feedback in each sprint meeting which proved to be invaluable to the project.

\newpage

\tableofcontents
\newpage

\listoffigures
\newpage

\section{Introduction}
\label{sec:introduction}

The project customer was Snowflake Software, a company that works in geographical data exchange services. The focus of this project is the exchange of aeronautical data between flight planners and data publishers such as the Federal Notam Service and the Aeronautical Information Service. Snowflake are planning to implement a data exchange service for clients that would allow consumers to receive aeronautical data in the form of `Notice to Airmen', using a push notification service. The group was given the task: ``To demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. In practice, this translated to exploring what architectures existed that could be used to implement a push notification system, and which of those would work with the requested standard of WSN.

As Snowflake were relatively unfamiliar with a number of the tools and technologies required to produce the service, the aim of the project from their perspective was to identify dead-ends and pitfalls a future team of theirs may fall victim to, as well as producing the groundwork for future implementations of the required standards. A number of possible architectures and tools were explored, with each Scrum meeting being used to recount all experiments and attempts as the process of producing the final product was as important as the final product itself.

\subsection{Terminology}
\label{sec:terminology}
\textbf{AES} - \textit{\textbf{A}eronautical \textbf{E}vent \textbf{S}ervice.} The name given to the system created over the course of the group design project.
\\
\textbf{NOTAM} - \textit{\textbf{No}tice \textbf{T}o \textbf{A}ir\textbf{M}en.} An XML format following the AIXM schema which notifies flight planners about events which could have an effect on their plan.
\\
\textbf{AIXM} - \textit{\textbf{A}eronautical \textbf{I}nformation E\textbf{X}change \textbf{M}odel.} A model designed to manage the communication of data in the field of aeronautics.
\\
\textbf{WSDL} - \textit{\textbf{W}eb \textbf{S}ervice \textbf{D}escription \textbf{L}anguage.} An XML-based language for defining the functionality of a web service.
\\
\textbf{SOAP} - \textit{\textbf{S}imple \textbf{O}bject \textbf{A}ccess \textbf{P}rotocol.} A protocol for exchanging information between web services.
\\
\textbf{CXF} - An open-source web service framework.
\\
\textbf{WSN} - \textit{\textbf{W}eb \textbf{S}ervice \textbf{N}otifications.} A specification for publish and subscribe services.

\newpage

\section{Project Description}
\subsection{Motivation}
\label{sec:intro_motivation}
Pilots need to be notified of hazards and events that may require them to make changes to their flight plan. Examples of such events includes: closed runways, restricted airspaces, extreme weather conditions, and even the presence of migrating birds through an airspace. Currently, flight planners receive such information by pulling all current NOTAMs, or `Notice to Airmen', and must manually sort through them to find only the notices which are relevant to their flight, which is extremely time consuming. The efficiency of this task could be improved dramatically if the filtering were performed automatically, using defined geographic regions and time frames. The aeronautics industry uses WSN as their preferred standard, so the customer required us to work to this standard or the alternative standard WS-Events.

\subsection{Customer}
\label{sec:intro_customer}

The customer for this project was Snowflake Software, a company focused on open data exchange, and various data mapping products. Within sprint meetings, the company was represented by Eddie Curtis, who expressed the feature requests and gave feedback, representing the product owner, whist Adrian McKenzie served as the Scrum Master, guiding sprint meetings and ensuring the team were able to work effectively and without impediment.

\subsection{Goals}
\label{sec:intro_goals}

The vision, as stated by Snowflake, was ``To develop a proof of concept push event service for delivering digital NOTAM updates to pilots; to demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. From this vision, the goal of the software produced over the course of the project was to explore options and find methods to filter the incoming data to a flight planner to only what is relevant by utilising a publish and subscribe model, removing the need for manual filtering and the pulling of specific relevant NOTAMs by pilots.

The subscriptions to this service had to be based on the relevancy of NOTAMs to the recipients. This relevancy was determined by geographic data, such as departure and arrival airports, and more complex temporal queries, for example, filtering NOTAMs to ones relevant only during the course of the flight. The software was required to use open standards for the publish and subscribe architecture, for example: OASIS Web Services Notification or WSN. The software also had to be capable of handling an estimated 10,000 messages per day. These messages come from two sources: the Federal NOTAM Service, which publishes a baseline NOTAM every 28 days, and updated delta files which arrive every 15 minutes.

\subsubsection{Project Goals}
\begin{enumerate}
\item Capable of parsing XML documents from the AIXM schema.
\item Capable of basic filtering of NOTAMs based on simple text fields such as airport names.
\item Capable of temporal filtering of NOTAMs.
\item Capable of receiving and handling subscriptions from consumers.
\item Capable of publishing parsed NOTAMs to relevant subscribers.
\item Produced utilising exclusively open-source libraries.
\item Using WS-Notifications to securely communicate all NOTAMs.
\item Able to handle 10,000 messages per day.
\item Able to poll the Federal NOTAM System to request all NOTAMs.
\item Store and update baselines to provide to consumers upon connection.
\end{enumerate}

\subsubsection{Additional Goals}
\begin{enumerate}
\item Spatial filtering of NOTAMs based on distances from points.
\item Spatial filtering of NOTAMs based on locations within bounding boxes.
\item Retrieve GPS co-ordinates for airports and airspaces from Snowflake's Global Aerodromes web service.
\item Enriching NOTAMs with GPS co-ordinates to allow for faster spatial filtering.
\item Handle cases where the Global Aerodromes service does not contain a designator for a location.
\end{enumerate}

\subsubsection{Project Scope Changes}
In the opening meeting, the initial project goals were set out by the Scrum Master, Adrian, and product owner, Eddie. Several meetings later, this list of requirements was updated, making spatial filtering a major focus for the project and temporal filtering an extension goal to complete if time permitted. As most of the groundwork for both filtering types could be reused, both were completed.

The storing and provision of baselines ceased to be a requirement after a few meetings, when it was revealed that Snowflake already possessed a service which provided updated NOTAM baselines and so this functionality was not required of any system produced in the course of this project.

A feature which was initially aimed for in the proof of concept design was polling of the FNS (Federal NOTAM System) for NOTAMs before filtering them and sending them out to the relevant subscribers. This was proved impossible, as access to the service for polling required registration with FNS which could not be done.

\subsection{Deliverables}
\label{sec:plan_deliverables}

The deliverables for the project were dictated both by the requirements of the customer and the module requirements. Snowflake required the production of a proof of concept service which fulfilled their requirements, whereas the module required several more deliverables.

\begin{itemize}
\item Functioning proof of concept service
\item Example project of service with consumer
\item Final group report
\item Final individual report
\item Progress seminar 1
\item Progress seminar 2
\item Final presentation
\item Group design project poster
\end{itemize}

The deliverables for the customer included both the functioning service which serves as a proof of concept for publish/subscribe-based NOTAM services and an example project for demonstrating its functionality in the final presentation.

In the first progress seminar, a description of the project goals was presented, along with plans for how to proceed from that point onwards. The second seminar contained a brief overview of the project for the second examiner, a description of all previous work completed and plans for the remainder of the time on the project.

\section{Background Information}

\subsection{NOTAMs}
\label{sec:bg_notams}
A `notice to airmen' (NOTAM), is a message sent to pilots and flight planners about events in a particular airspace or at a particular airport. The events may be hazards that have to be avoided, such as dangerous weather, or updates on runway closures, which may or may not be relevant to the flight. They're used by pilots and flight planners to notify them about events that might affect the safety of their flight. NOTAMs are distributed by various aviation authorities around the world and are generally received by their consumers via pull notifications.

The NOTAMs are delivered as deltas from a baseline of data. For instance, the latest baseline data may indicate that a particular runway has been closed indefinitely but a pilot may be told via a NOTAM that the runway has been reopened, overriding the baseline information. Deltas can be either temporary or non-temporary; temporary updates apply for a certain time frame, whereas non-temporary updates last indefinitely. If there are many delta updates, it is often useful to make a snapshot. A snapshot is an updated baseline that is formed from the original baseline and a combination of deltas, this allows flight planners requesting a NOTAM after a large number of delta updates to quickly receive up to date information without having to request large numbers of delta updates.

All NOTAMs are sent as XML documents using a schema known as AIXM. The AIXM, or \emph{Aeronautical Information Exchange Model}\cite{aixm}, is a specification for aeronautical information flows and includes two major components: the AIXM conceptual model and its corresponding XML schema. The conceptual model is used to model all important properties, features, and restrictions which govern all aeronautical information. Features include runways, aerodromes, and airspaces, whereas properties can include information such as the width and length of a specific runway. This model is implemented as the AIXM XML schema which all NOTAMs are created using.

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth}
\includegraphics[scale=0.5]{aixmUML.png}
\end{center}
\caption{UML diagram highlighting some features of the AIXM conceptual model.\cite{aixm}}
\label{fig:aixm}
\end{figure}

\newpage

\subsection{Publish/Subscribe Architecture}
\label{sec:bg_pubsub}

\begin{figure}
\begin{center}
\def\svgwidth{\columnwidth}
\input{broker.pdf_tex}
\end{center}
\caption{Publish/Subscribe Architecture with Broker}
\label{fig:pubsub}
\end{figure}

Publish-subscribe is a messaging architecture that allows decoupling between the \emph{producers} and the \emph{consumers} of \emph{messages}. Messages are categorised into \emph{topics}; producers publish messages on a given topic without requiring knowledge of which (if any) subscribers the topic has, and consumers subscribe to a given topic with no knowledge of which (if any) producers are publishing messages with that topic.

This architecture is generally implemented using a \emph{broker}, which exists as middleware between the producers and consumers. Producers send messages on a given topic to the broker, which then decides which consumers the messages should be forwarded to. Consumers subscribe via the broker to topics they’re interested in, and the broker forwards relevant messages to them. This architecture has a number of benefits, including providing loose coupling between producers and consumers (since they need no knowledge of each other to make use of this architecture) and facilitating scalability by supporting arbitrary numbers of producers and consumers for any given topic.

In Java, brokers typically implement the Java Message Service (JMS) API provided by Java Enterprise Edition\cite{hapner2002java}, which offers a consistent interface for communicating with many different message broker implementations.

This architecture can be used to turn a pull notification system into a push notification system by creating a publisher (or several publishers) that regularly polls and retrieves messages from the existing pull notification system, and then sends them to the broker (which pushes them to the appropriate consumers).

\subsection{Web Services Notification}
\label{sec:bg_wsn}

Web services notification (or WSN) is a set of specifications created by OASIS\cite{oasisWSN} that defines the interactions between web services, using notifications, which can form the basis of a publish/subscribe architecture built using web services. They provide a standardised way to communicate with and between web services, without having prior information about them. It includes facilities for detailed subscription, where those wishing to consume information can register with a web service along with details about the type of information they wish to receive. WSN was designed to revolve around these subscription details known as ``topics'' and use them to filter down subscriptions, allowing for large scale usage of the publish/subscribe architecture without unwanted message overload.

WSN covers a family of smaller specifications including WS-Base Notifications, WS-Brokered Notification and WS-Topics. WS-Base Notifications is a specification of the simple producer/consumer relationship, consumers send out subscription requests to the producers, specifying the subset of messages produced by that producer which it wishes to receive. The request also contains a reference to the consumer itself, so the producer knows where to send the messages.

WS-Brokered Notification works in a similar manner to WS-Base, but introduces a middleman between the message publishers and the message subscribers known as the broker. The broker is designed to decouple the connection between producers and consumers and allows for advanced messaging features such as demand-based publishing, load-balancing and publication of messages from entities which are not themselves web service providers.

WS-Topics is a specification which defines a method for categorising subscriptions into ``topics''. This specification is designed to work in conjunction with WS-Base or WS-Brokered and contains various methods for defining groups of subscriptions.

WSN is a favoured standard of the aerospace industry, which is why it is highly desired that the final product uses it.

\subsection{Web Services Description Language}
\label{sec:bg_wsdl}

Web Services Description Language (or WSDL) is an XML based language for describing the interfaces and functionality of a web service\cite{christensen2001web}. A WSDL file provides a machine readable description of a web service's interface, including the function calls, parameters they expect and return values.

Typically, it is used in combination with SOAP to provide a web service, where a client program will read the WSDL, to determine the functions available, and then use SOAP to call them\cite{curbera2002unraveling}.

\subsection{JAXB and DOM Parsing}
\label{sec:bg_parsing}

JAXB, or ``Java Architecture for XML Binding'', is a tool used to marshal Java objects into XML and unmarshal XML into Java objects. It can be used as an alternative to standard XML parsers, taking complicated schema and converting XML created from them into usable Java objects\cite{ort2003java}. Use of JAXB was explored towards the end of the project for use in spatial filtering. It was intended to use JAXB to parse XML filters of the OpenGIS standard in order to create filters within the system, however, implementing JAXB proved beyond the scope of the project in terms of time, given the nature of the system as a proof of concept piece with a strict time constraint. Instead, a more commonly used XML parsing method known as ``Document Object Model'' or DOM parsing was settled on.

DOM parsing is one of the two most commonly used XML parsing methods, the other being SAX parsing. A DOM parser works by loading the entirety of an XML document into memory, modelling it as a tree to allow it to be easily traversed. The tree is traversed node by node to obtain the required information, in this case known tags within the OpenGIS standard for use in spatial filtering. This method of parsing generally has quite a large overhead when parsing large XML files, due to the fact it needs to parse the entire file.

\subsection{XML Catalogues}
\label{sec:bg_catalogues}

XML files contain references to external XML resources such as schema files and WSDL files, these external resources are also able to reference other files, which can lead to long chains of resources and massive downloads. When referencing a large number of external files, events such as network outages and slow connections can cause major issues for a service and its performance. Another problem which can occur when referencing external resources is relative URLs being used to address them. Relative URLs limit the service using them to the context in which it was created, as launching it from any other location will not work.

A solution to these problems is creating an XML catalogue, which allows these files to be stored locally and a service to be instructed to use these local references when its external counterpart is requested. The catalogue maps between these two versions of the same resource.

\subsection{JIRA Issue Tracking}
\label{sec:bg_jira}

Issue trackers are systems which maintain lists of issues in an ongoing project, with the ability to assign these issues to specific group members, report bugs while working and connect solved issues to source control commits which fix them. Issues can also be broken into sub-tasks, prioritised and categorised to improve efficiency. For example, many issues were recorded after each sprint meeting, these included work for the next sprint, work for near future sprints and overall project goals. These issue reports were categorised according to the sprint which they were to be completed in. When every issue attached to a sprint was marked as solved, a sprint was considered complete in terms of work.

As the customer is a company which regularly works on projects of a much greater scale than that of the AES system, they possess a number of professional tools for organisation and issue tracking. One of these is JIRA, an issue tracker created by Atlassian designed to work with the chosen cycle of creating user stories and acceptance criteria with the customer and assigning these stories to team members after each sprint meeting. As JIRA is a professional piece of software which normally requires a monthly fee for usage, it had to be accessed via the server running it licensed to the customer through their VPN. By using the tools in use by the customer, the product customer was able to easily keep track of progress and make comments on submissions to the service.

\subsection{ActiveMQ}
\label{sec:bg_activemq}

Apache ActiveMQ is an open source message queue service which communicates using the Java Message Service (JMS) \cite{activeMQ}. It contains a number of features deemed useful to the project including subscription management and filtering. The filtering itself is through objects known as selectors, which use the SQL 92 syntax to filter messages. SQL 92 is a fairly barebones SQL syntax, which features basic arithmetic operators (addition, subtraction, multiplication, division and modulo), enough for the requirements set out by the customer of temporal filtering and spatial filtering using GPS co-ordinates.

\subsection{Git and GitHub}
\label{sec:bg_git}

Git is a distributed version control system, which allows multiple people to work on a single project at the same time, without necessitating the use of a shared network, as well as tracking document history, affording reversions or rollbacks to previous versions. Some form of source control is essential for effective collaboration between programmers and greatly aides coordination and communication.

GitHub is a Git repository hosting service, which also offers other features that aide collaborative coding, such as issue tracking and task assignment, which can aide greatly in project management.

\subsection{Travis CI}
\label{sec:bg_travis}

Travis CI is a free, hosted, continuous integration platform that is used to automatically build and test projects hosted on GitHub. It automatically detects when a commit has been pushed to a linked GitHub repository and attempts to build all branches and run tests. It can then be set up to show the results of this in the GitHub readme or to notify the developer in some other way, such as by emailing the results.

This practice of continuous integration allows developers to detect problems quickly. This also makes fixing them easier, as less back tracking is required to identify when and where an issue occurred, reducing the number of large scale revisions required. It also means they can assure the software will run on a variety of computer set-ups, without time consuming manual testing\cite{fowler2006continuous}.

\subsection{Cucumber}
\label{sec:bg_cucumber}

Cucumber is a testing suite that runs behaviour-driven development style acceptance tests\cite{cucumber}, i.e., tests written to check if the requirements of a specification are met by the code. These focus on testing behaviours, i.e. the action the code is supposed to perform, where each test goal is independent of the code’s structure, rather than an approach like unit testing, that tests individual units of code.
A single Cucumber test consists of a plain text feature definition, written in plain English, and corresponding step definitions, that is, the code that actually tests the feature.

A feature definition describes a single behaviour the code is supposed to be able to perform (such as ``Feature: Filter NOTAMs by time'') and then outlines one or many scenarios: sets of conditions that test a facet of the behaviour. These describe initial conditions, steps that are then carried out, and what the results of performing those steps (with those initial conditions) should be. These are specified with the keywords ``Given, When, Then'', i.e., given a set of initials conditions, when something happens, then the program should do something (with the ability to have multiple steps with the keyword And).

For each feature, there is then a set of corresponding step definitions, that is, code to test the feature, in the form of a subroutine for each step (e.g. there will be a single subroutine for ``Given a broker is running'', which will check that a broker is running and, if not, launch one). Depending on the keyword, these should either set up the initial conditions (in the case of Given or a following And), perform a particular action (in the case of When or a following And), or assert that particular conditions are true (in the case of Then or a following And). If all conditions are satisfied, the test passes, otherwise it fails.

In this project, feature definitions were based on the client's user stories, keeping the tests focused on satisfying the customer's requirements.

\subsection{SOAP}
\label{sec:bg_soap}

SOAP (or, originally, Simple Object Access Protocol) is a platform independent protocol for exchanging information between web services, based on XML, and is dependant on other application layer protocols for message transmission (typically HTTP but other transport protocols are supported as well)\cite{curbera2002unraveling}.

\subsection{Netcat}
\label{sec:bg_netcat}

Netcat is a utility to read from and write across network connections using TCP or UDP. It is designed so that it can easily and reliably be used as a back-end tool, so it can be used by other programs. It is frequently referred to as the `Swiss army knife of networking' because of its wide array of features, such as port scanning and a large number of network debugging and investigation tools. In this project, it was used to retrieve data from a WFS and to implement a mock consumer, for demonstrative purposes.

\subsection{cURL}
\label{sec:bg_curl}

cURL is both a library and command line tool for sending and retrieving data using a wide variety of protocols (such as HTTP, FTP, SCP, POP3, etc.). It is very portable, working on a wide variety of platforms, and bindings are available for a large number of programming languages. It was used to send subscription requests from the consumer.

\subsection{Open Geospatial Consortium}
\label{sec:bg_ogc}

The Open Geospatial Consortium (or OGC) is a voluntary standards organisation specialising in publicly available interface standards for geospatial services and content, and GIS data processing. It consists of over 500 universities, companies, and government agencies and have defined more than 30 standards; this includes WFS and GML, which are of interest to this project and are described below.

\subsection{Geography Markup Language}
\label{sec:bg_gml}

Geography Markup Language (or GML) is an XML grammar designed to express geographical features. It serves as both a geographical modelling language and as an data interchange format for the exchange of geographic data on the internet. It includes a very wide range of primitive objects, so that it can be used to express and integrate all forms of geographic information together. It was the standard used to encode the airport information that was used to perform NOTAM enrichment.

\subsection{Web Feature Service}
\label{sec:bg_wfs}

The Web Feature Service interface standard (WFS) is a standard by the Open Geospatial Consortium that provides a platform-independent web interface for requesting geographical features. By default, the data is encoded in the GML format, although other formats can be used. In this project, Snowflake Software's GO Publisher WFS was used to retrieve the airport data. WFS includes the ability to query features based on spatial constraints, so it could be queried to only return airports within a particular area, allowing program testing to be performed with more manageable amounts of data.

\newpage

\section{Planning and Organisation}
\label{sec:planning}

\subsection{Software Development Methodology}
\label{sec:plan_software_development_methodology}

The project was built using the Agile software development practices of Scrum, with each sprint following a similar structure. Every other week, the team would meet at Snowflake Software's main offices. First, progress in the previous sprint was discussed, then the customer came up with ideas for features which could be worked on in the following sprint. Next, user stories based on these features were created, dividing up the requirements of each feature and helping to create acceptance criteria which would come together to prove acceptance that the feature was successfully implemented.

\subsubsection{Meetings}

At the beginning of each sprint, a sprint meeting was held to both close the previous sprint and prepare for the following one. First, features implemented in the previous sprint were demonstrated to the customer, allowing them to track the progress of the project and decide if the feature had been successfully implemented. In the event that the customer was not satisfied with the work done in the sprint, the user stories were kept in the backlog for the following sprint. Otherwise, the customer discussed future features to be implemented and broke the feature up into user stories to be added to the backlog.

Meetings also regularly featured exercises to identify issues and find solutions to problems faced by the group over the previous sprint. Before moving onto work that needed to be done for the following sprint, each group member and the customer filled in a table of three columns on a whiteboard. These columns listed points members felt happy about in terms of project progression, points unsure about and points the members felt required discussion. This list was then prioritised and discussed in order, finding solutions for problems such as group dynamic issues and ensuring the practises members agreed were going well were maintained. These exercises not only created a more open forum for members of the group to start discussions about things they might otherwise be uncomfortable discussing, but allowed the customer to maintain awareness of aspects of the project which were working well.

\subsubsection{Tools}

Various software was used to aide this process, including JIRA (access to which was provided by the client) and Cucumber. JIRA is an issue tracker that manages user stories and their corresponding acceptance criteria as per each feature. User stories within JIRA are stored in the standardised \emph{``As a, I want, So that''} format, with their acceptance criteria written as \emph{``Given, When, Then''}. This format is also used by Cucumber for acceptance tests. Team members on JIRA can choose what user stories to complete or they can be assigned.

Early on in the project, the group was given access to JIRA on Snowflake's private servers. This allowed control over the backlog to be maintained and a record of what had been completed to be kept. However over the course of the project, the number of steps required to connect to the Snowflake VPN and access JIRA, as well as the difficulties arising from never having used the service before, it was decided that keeping track of the project would be better done through GitHub, which was already being used as a code repository. The issue tracking features within GitHub replaced JIRA's extensive features (the majority of which far surpassed the requirements of a small project such as this) and all tasks became issues, still using the standard user story format.

\subsection{Skills Audit}
\label{sec:plan_skills_audit}
To maximise the efficiency of the team and to minimise the amount of time which needed to be spent learning new skills for the project, it was necessary to assign tasks according to relevant skills and experience. In order to do this it was necessary to produce a skills audit (shown below) and divide tasks according to it.

\begin{table}[h]
\begin{tabular}{|l|l|}
\hline
\emph{Team Member} & \emph{Relevant Skills}                           \\ \hline
Andrew Sullivan    & Java, Python, JUnit                              \\ \hline
Charles Sherman    & Java, WSDLs, SOAP-UI, JUnit, Cucumber, Scrum     \\ \hline
Peter Prince       & Java, Python, JUnit, cURL, Netcat                \\ \hline
James Robinson     & Java, Maven, Git, JIRA, Travis, Cucumber, JUnit  \\ \hline
\end{tabular}
\end{table}

During each sprint meeting the work for the following sprint was decided upon, followed by a meeting in labs to break down the sprint work further and assign each key area to a team member according to their relevant skills. Due to the nature of the project, a lot of early work was not easily divisible, so a system of a single team member coding as the rest of the team researched and provided support, was used.

\subsection{Risk Analysis}
\label{sec:plan_risk_analysis}

In order to minimise the effects of problems which were likely to occur over the course of the project, precautionary measures were put in place to deal with these eventualities in advance. The most obvious risk was data loss, but factors such as the project being likely to involve the handling of sensitive information had to be considered.

\subsubsection{Data Loss}

To prevent the loss of code or data, a combination of the code repository GitHub and Dropbox were used to keep backups of both data and code. Individual team members used GitHub to commit code for both transfer between machines and to create an online backup in case of computer failure.

\subsubsection{Sensitive Data}

As the project included working with data of a possibly sensitive nature, the security of any data was a factor which had to be considered. Partway through the project a set of NOTAMs was provided by the customer to test the system with, however the contents of the NOTAMs were considered to be classified. As the project's code and resources were stored on a publicly available code repository, these sensitive files were stored separately on a private Dropbox created previously.

\subsubsection{Team Member Absence}

In the event of a team member not completing work assigned to them, the work would be transferred to others who had already finished the tasks they were originally assigned. The team had twice-weekly, group workshops where most of the code for the project was written; however timetable issues meant that not everyone could always attend. People who were able to attend the workshops were assigned tasks which were more linked and would benefit from the team communication available at a workshop. Tasks which could be completed on their own were assigned to absent team members.

\subsection{Customer Relations}
\label{sec:plan_customer_relations}

The project customer was Snowflake Software, a Southampton-based company specialising in open data exchange solutions. Over the course of the project, regular sprint meetings were held with a number of Snowflake employees, including one of the CEOs, Ian Painter; the chief technology officer, Eddie Curtis; the professional services director, Alex Brooker; head of software development, Adrian McKenzie (who acted as the main liaison with the company), as well as a number of members of their development team who were present in the final presentation to the company of the finished product. Communication with Adrian was done through both emails and the fortnightly Scrum meetings.

\subsection{Version Control}
\label{sec:version_control}

The Git version control system was selected for this project because it was the version control system that the team was most experienced with. A number of services exist for Git hosting, perhaps the most well-known of which are GitHub and BitBucket. For this project, the team elected to use GitHub to host the repository primarily because of its simpler issue tracking system. The project was stored in publicly accessible, open-source repository with the permission of the client.

At the beginning of the project, a simple Git workflow was decided upon in which new features were developed in their own feature branches, and then merged by the maintainer of that branch into the \texttt{master} branch. Small features or bugfixes that did not need more than one commit were directly committed to the master branch. Unfortunately this workflow provided no method of code review before a given commit was merged into the \texttt{master} branch, meaning that commits that didn't compile or run properly, and noisy commits with lots of whitespace/indentation changes were commonplace.

Our client suggested switching to the Git Flow branching model \cite{Driessen2010}, which advocates developing new releases in a dedicated \texttt{develop} branch, and only merging releases into the \texttt{master} branch. Features are, once again, developed in separate feature branches before being merged into the \texttt{develop} branch. This model has the advantage that by developing in a separate branch and only merging when a release is made, each commit in the \texttt{master} branch can be considered to be stable and production-ready. However, as it was only developing a basic prototype being developed for the project (and would therefore not be making any releases), the distinction between the \texttt{master} and \texttt{develop} branches seemed redundant.

After watching Zach Holman's presentation \emph{How GitHub Uses GitHub to Build GitHub} \cite{hgugtbg}, the team opted to use a similar git workflow for the project. Under this model, features are again developed in separate branches, and once they are complete the branch owner opens a \emph{pull request} to merge that branch into the \texttt{master} branch. Every other member of the team is then able to perform code review, comment on the pull request and suggest improvements. In addition, Travis automatically builds and tests the pull request. Once every team member is satisfied with the pull request, it is merged and closed. A pull request from the project can be seen in figure~\ref{fig:pull_request}.

\begin{figure}
  \includegraphics[width=\linewidth]{pull_request.png}
  \caption{Example pull request}
  \label{fig:pull_request}
\end{figure}

\subsection{Testing Methodology}
\label{sec:plan_testing_methodology}

Behavioural-driven development style acceptance testing was used, utilising Cucumber to express the acceptance criteria for each story and allowing them to be converted into automated acceptance tests which could be run using JUnit. The scenarios were written such that the implementation of a feature can change without the file needing to be rewritten. For each story, when all acceptance tests had passed within JUnit, the user story was removed from the backlog. This type of testing keeps the focus on user stories, testing features rather than abstract units of code, better ensuring that the code satisfies the client's requirements rather than some abstract measure of performance.

Continuous integration was also used, utilising Travis CI. This helps to ensure new code doesn't break old tests, ensures the software can run on a variety of computer set-ups, and allows problems to be detected more quickly and fixed with less backtracking.


\newpage

\section{Implementation}
\label{sec:implementation}

Due to the exploratory nature of the project, there was a lot of work that went into the project that wasn't present in the end result. Work was completed using the Scrum methodology over the course of five sprints. In the this section of the report, each sprint is discussed individually to show the alternatives explored and to show the work that didn't make it to the final prototype as well as highlighting the motivations for the choices made in the development of the software.

\subsection{Sprint 1}
\label{sec:impl_sprint_1}

The initial meeting with Snowflake introduced the project and the scope, requirements and overall goals of the system. Snowflake described the vision of the project, the goal set was: ``To demonstrate the feasibility of using Publish/Subscribe to communicate relevant NOTAMs to flight planners''. This also had to be implemented using either the WS-Notification standard or WS-Events, as these are the most used standards in the industry.

\subsubsection{Sprint Goals}

For the first sprint, the goal was to have a basic push notification system running that could publish and receive an XML.

The customer had no preference for what language the project was written in. The team chose Java because of the libraries and tools available.

\userstory{Basic Push Notification}{flight planner}{to be notified if there is a change to my destination airport for my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

To implement the user story required setting up Apache ActiveMQ \cite{activeMQ} and implementing a method of providing a message to the broker and a way of displaying the message so tests could be written to see that it was received correctly.

\subsubsection{Work Allocation}

As the sprint goal was to create the foundations of the code with modules that were very tightly linked, it was difficult to separate the work between team members. The majority of the coding work was done by James, whilst everyone else in the group assisted by researching the possible tools that could be used. Once a solution that worked had been found, James presented the code to everyone else so that the group understood the chosen implementation and the motives behind it. Other team members were also assigned non-coding tasks such as setting up the Git repo and using JIRA.

\subsubsection{Implementation}

After working on a basic implementation of ActiveMQ and exploring the documentation available online, it became obvious that it contained no implementation of WS-Notification and the page within its online documentation which was supposed to detail its implementation was a dead link. One of the requirements stated by the customer was the inclusion of WS-Notification and whilst this was a soft requirement, meaning that it's inclusion could be forgone if sufficient justification was given, the project was in an early enough stage at this point in development that it was worth exploring other alternatives; alternatives such as developing a WS-Notification implementation from scratch using CXF.

Apache CXF is an open source framework for producing web services, and after ActiveMQ was deemed unusable, research began into using CXF. Work started by creating a WS-Notification implementation within CXF. Progress in this was slowed significantly by documentation for the framework being extremely limited. Whilst creating this implementation, it was discovered that an implementation already existed, undocumented, within Apache CXF. Through experimentation and knowledge gained from attempting to build a bespoke implementation, a basic demo was produced, subscribing a consumer to a preallocated topic and sending out a message through the subscription.

\subsubsection{Review}

At the end of the first sprint, the first user story had been completed. The code used Apache CXF with the WS-Notification standard that came included with it. This implementation was capable of sending a NOTAM to a hard-coded consumer who also had a hard-coded subscription to a broker. The subscription method used simple string topic names, for example a topic would be published with an aiport name ``AIR1'' and a consumer with a subscription ``AIR1'' would receive it. This implementation was not capable of doing any more complex filtering as was required later in the project which meant the system had to be changed in future sprints.

The major issue in this sprint was the bottleneck caused by the coding. It was impossible to divide up the programming work between group members, which made it difficult to progress through the work quickly and to keep everyone informed about the work. Although it was inefficient, by having one team member write the code whilst everyone else researched, this did mean that everyone was at least familiar with the project so that they would be able to work efficiently in future sprints. At the end of the sprint, the team had a meeting without the customer, where the code was reviewed, so that everyone understood it and could work on it in the future sprints.

There were no automated tests written for the first sprint. Since values were hard-coded for the broker address and the message topics, there was only one route through the code. For this sprint, code was manually tested, and did not write automated unit or system-level tests. As the code expanded in future sprints, automated tests were added to check for errors in the code when changes were made and to demonstrate to the customer that the project was working.

There were issues with writing the user stories to Jira. Peter and Andrew were the only people who could access the online service due to issues with the web plugin on other team member's machines. This made the point of assigning work to group members on Jira useless, as people couldn't see what work they had been given. This issue was solved in the next sprint where Jira was replaced with GitHub issue tracking.

\subsection{Sprint 2}
\label{sec:impl_sprint_2}

\subsubsection{Sprint Goals}

In the second sprint, the goals set were to expand on the work completed in the first sprint. At the end of the first sprint, a producer, broker and consumer had been written, all with hard-coded values for the subscription information. The target for the end of the second sprint was to have a consumer that subscribed with configurable topics, and to have multiple consumers listening to a broker. As the objective of the system was to create a proof of concept, it was also important to develop tests to demonstrate to the customer that the software was working.

\begin{table}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \emph{Origin}         & \emph{Destination}    & \emph{Route}                 & \emph{Consumers} \\ \hline
    One Hard-Coded & None           & None                  & One       \\ \hline
    Configurable   & One Hard-Coded & One Hard-Coded        & Two       \\ \hline
    ~              & Configurable   & Multiple Hard-Coded   & Multiple  \\ \hline
    ~              & ~              & Multiple Configurable & ~         \\ \hline
    \end{tabular}
\caption{Sprint 2 Table}
\label{tab:sprint2}
\end{table}

During the second sprint planning meeting a table of requirements was made with Snowflake. The table split the tasks into layers that had to be implemented row by row, but within each row the columns were independent and could therefore be completed in any order. At the start of the sprint, the top row of the table had already been completed and the target for the end of the sprint was to have the lowest row from each column implemented. The process works by filling out each layer of the table, one at a time to make sure features could be implemented in feasible steps.

\userstory{Notification Filtering By Airport}{flight planner}{to be notified if there is a change to my start, end or airports on the route of my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

\subsubsection{Work Allocation}

Unlike in the first sprint, the project now had enough complexity for programming work to be divided effectively. Peter and James worked on rewriting the code so that multiple subscriptions were possible and could be configured by command line arguments. Charles was assigned writing tests for the system using Cucumber.

\subsubsection{Implementation}

In practice it was found that the jump from hard coded values to fully configurable code could be done in a single step. The difficulty in making the step was in creating the tests for the new configurable code. A set of Cucumber tests was created to check that a NOTAM could be sent from a producer, to a broker and out to a set of consumers. Consumers had individual subscriptions so it could be demonstrated to the customer that consumers only recieved messages they were subscribed to.

\subsubsection{Review}

At the end of the sprint the sprint goals were all met. The system was now capable of having multiple consumers with unique subscriptions and the produced could send out multiple NOTAMs which were not hard-coded into the system.

The tests at the end of the sprint were found to contain bugs. This made the tests counter-productive as they would sometimes give the impression that the system was working when it would only work on one machine, in a specific set of circumstances. There were also issues with setting up and pulling down services between tests, which meant that tests had to be run one at a time to prevent issues with services such as the notification broker. Additionally, since the tests ran every time the code was compiled this slowed the development process considerably when long timeouts for the tests had to be included. In sprint 3, the tests were moved to Travis CI, which solved these issues. At the sprint meeting with Snowflake, the system was run manually rather than using the Cucumber tests to evidence that the code was working.

\subsection{Sprint 3}
\label{sec:impl_sprint_3}

\subsubsection{Sprint Goals}

In the planning meeting for this sprint Snowflake mentioned JBoss A-MQ as a possible alternative to the code that had already been written. JBoss A-MQ is a fork of Apache ActiveMQ that also contains the Apache CXF implementation of WS-Notifications. It was hoped that swapping to using this system instead of the code already written would allow for the implementation of more complex filtering on messages.

The main focus of sprint 3 was to research the extent of JBoss A-MQ's filtering capabilities, to assess its viability for use in the project. Through research in the previous sprint, the prominence of JBoss as a message queue used industrially had become apparent. Research done by Snowflake themselves also suggested that it would be useful for the project.

The subject of testing was also brought up, and the implementation of a continuous integration system was decided upon to streamline the implementation of changes to the project. Early on in the sprint, Jenkins was implemented and set up to run tests continuously on the latest version of the project, updating the project's GitHub page with the outcome of the latest tests.

It was in sprint 3, after the progress made up to that point was shown in the sprint meeting, that another representative of Snowflake expressed the importance of spatial filtering over temporal filtering. The utility of spatial filtering is obvious in the case of flight planning, a flight planner simply defining the start and end point of the journey and then receiving all messages relevant to that geographical space would save considerable time when compared to the alternative of manually subscribing to airspaces and airports which are within range. Temporal filtering would allow a consumer to only receive NOTAMs that are in effect for the duration of their flight, dramatically reducing the number of irrelevant NOTAMs being sent.

In order to perform spatial filtering, latitude and longitude data for every airspace and airfield would be required, however NOTAMs lack information at this level of detail so a solution to this problem had to be found. The information required for temporal filtering is featured in each NOTAM, and only conversion to a usable time stamp format was required.

\userstory{Notification Filtering By Time}{flight planner}{to be notified if there is a change to airports I'm interested in during my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

\userstory{Notification Filtering By Geography}{flight planner}{to be notified if there is a change to airports on the route of, or near my planned flight.}{I can change my plan in response to events involving airspaces and runways on my route.}

\subsubsection{Work Allocation}

Charles continued working on the Cucumber tests, writing more case scenarios and fixing bugs in the old tests. He also set up the GitHub repository to work with the continuous integration service on Travis. James was assigned  the task of moving the system over to JBoss A-MQ; however in the middle of the sprint it became apparent that JBoss A-MQ wasn't capable of complex message filtering, so James then started to work on a JMS system. Peter researched temporal and spatial filtering and was set the task of writing a basic test scenario to check basic filters based on GPS co-ordinates could be produced.

\subsubsection{Implementation}

The main focus of the sprint was implementing temporal and spatial filtering of messages. As mentioned in the first sprint, the current system could only filter subscriptions based on strings and was not capable of filtering on queries for example listening to all messages published from a given area.

Work was split into exploring documentation of JBoss A-MQ for implementation instructions and information regarding filtering. After a week of investigation, it became apparent that JBoss did not have the capabilities to fulfil the requirements of the product. JBoss filtering  was also restricted exclusively to filtering on strings.

The inadequacy of JBoss A-MQ left two options: either forking JBoss A-MQ and modifying it to support the required features, or dropping it in favour of a message queue which does fulfil those requirements. Basing the decision on the scope of the project and limited time remaining to complete it, it was decided to abandon JBoss A-MQ. As it was early in the sprint and the outcome of the decision would dramatically affect the outcome of the sprint, the customer was notified of the decision, to ensure it was satisfactory to them.

The decision was made to swap to using JMS and a filtering proof of concept was created to justify the change. Using JMS Selectors to filter based on dummy GPS co-ordinates randomly assigned to each NOTAM, a basic implementation of spatial filtering was created. NOTAMs were also given dummy time codes that were simple integer values and filtered based on those to create a form of temporal filtering.

\subsubsection{Review}

In this sprint, a major shift was made from JBoss A-MQ to a JMS-based service. This change required a large amount of work to complete, but meant that it would be possible to implement complex filtering methods that were required by the customer. Basic spatial filtering was created and filtered NOTAMs based on dummy GPS co-ordinates. To expand on the system would require NOTAM enrichment, where a NOTAM is parsed and extra information such as geographic and temporal data is added, before sending the NOTAM on to the consumer.

In making this change to the code, WS-Notification support was lost. This made re-implementing WS-Notification support a priority for the next sprint as it was a requirement of the customer.

\subsection{Sprint 4}
\label{sec:impl_sprint_4}

In the fourth sprint meeting the topic of NOTAM enrichment was raised. Many NOTAMs contained no geographic data or no temporal data which made filtering impossible, this made the enrichment of the NOTAM data a requirement for our software to filter the data. Snowflake provided access to a HTTP-GET web feature service (WFS), which returns latitude and longitude locations for all European airfields which could provide our software a means of enriching NOTAMs with geographic data by looking up the airport name in the NOTAM and finding its corresponding location.

Ideally, calling this service would be implemented into AES itself, however given the time remaining on the project and the status of the product as a proof of concept, it was elected to simply retrieve data for all airfields and hard-code them into the system for use in spatial queries, knowing that the system could easily be modified to directly call the service and utilise the data if more time was spent on it.

\subsubsection{Sprint Goals}

After JBoss A-MQ proved to be a dead end, a new system structure was decided on, with only the consumer side using WS-Notifications to communicate. All remaining communication between brokers and producers was changed to rely on JMS, with converters between WS-Notification topics and JMS filters becoming a primary focus for the sprint.

As well as re-implementing WS-Notifications, the system also had to be expanded to include NOTAM enrichment so that spatial and temporal filtering could be done with real data. The previous code which used dummy temporal data had to be changed to use real timestamps and the fake geographic data was to be replaced with real data from the HTTP-GET service provided by Snowflake.

\subsubsection{Work Allocation}

For this sprint, Charles was assigned the task of building more elaborate temporal filtering; in parallel to this, Peter built upon his previous work on spatial filtering. James was tasked with implementing the WS-Notifications standard into the system and Andrew created an XML document which contained a subset of all airport locations, narrowing them to just European aerodromes. Due to the large number of global aerodromes, it was necessary to reduce this just to relevant locations in the interest of parsing time.

\subsubsection{Implementation}

Temporal filtering was implemented by parsing the NOTAM XML and converting the timestamps in the data to a Unix time stamp format. The time stamp measures the time in a single number counting milliseconds. Using Unix time stamps, it is possible to directly compare the times using inequality operators. The code allows a consumer to subscribe to topics after a given time, or within a time period.

The selectors used for filtering are based on SQL 92, which features only simple arithmetic operators. In order to create a proper bounding box which covered a plane route, more operators were required (such as square roots, which are not included). As a result, the spatial filtering implementation was simplified to an axis-aligned bounding box. This type of spatial filtering required simply greater than and less than operators to perform.

To produce the subset XML document, a bounding box was placed over Europe and submitted to the Global Aerodromes service, all aerodromes within the given bounding box were retrieved.

\subsubsection{Review}

Unfortunately, the entire WS-Notification specification was not implemented in this sprint, however the code for filtering and enriching the data was completed. The task of creating a WS-Notification was much larger than previously thought and although it was planned to only implement a small section of WS-Notifications, this was still too much work.

\subsection{Sprint 5}
\label{sec:impl_sprint_5}

\subsubsection{Sprint Goals}

The goal for the final sprint was to produce a complete minimal viable product for the product owner. This involved finishing the WS-Notification implementation, packaging the application as a WAR for deployment on Tomcat, building a WSDL for the web service, making a barebones client to demonstrate the service's functionality, and implementing basic OpenGIS filtering support. It was also planned to replace the DOM parsers with JAXB, in order to make it easier to implement the OpenGIS filtering support.

\subsubsection{Work Allocation}

OpenGIS filters were needed to filter temporally and geographically within WS-Notification. This required small changes to the enrichment of the NOTAMs which was assigned to Peter and Andrew.

The service had to be deployed as a WAR on Tomcat for consumers to communicate with to create subscriptions and to receive relevant NOTAMs. The deployment was assigned to Charles and the mocking of a consumer was assigned to Peter.

To parse the filters, James looked into using JAX-WS however this was not used in the final software. Charles and James also worked on generating a WSDL for the producer and the consumers of the WS-Notification messages.

% \begin{tabular}{l|l}
%   \emph{Task} & \emph{Assignee(s)} \\ \hline
%   JAX-WS code generation using CXF Maven plugin & James \\
%   WSDL for AES' WS-Notification implementation & James, Charles \\
%   WAR generation using Maven & Charles \\
%   Tomcat setup and WAR deployment & Charles \\
%   Mapping from OpenGIS filters to JMS selectors & Peter \\
%   Mock client implementation using NetCat and cURL & Peter \\
%   Barebones client implementation using JAX-WS & James \\
%   NOTAM enrichment & Peter, Andrew \\
%   Parsing the Snowflake GlobalAerodromes WFS & Andrew \\
% \end{tabular}

\subsubsection{Implementation}

The Apache CXF JAX-WS codegen plugin for Maven was used to build class stubs from the WSN WSDLs. Unfortunately, the WSDL's provided by Oasis did not specify any endpoints, and so the WSDL's were created from scratch based on a subset of the WSN WSDL included with the Apache CXF implementation of WSN.

A mock client for the system was implemented using NetCat and cURL - NetCat took the place of the WSN consumer, listening on port 80 and simply printing out all received data to standard output, and cURL was used to send a subscription request to the WSN broker. Unfortunately, the correct format for the subscription request couldn't be determined, so the barebones client implementation was abandoned and a new implementation using CXF was created, based on the WSDL for the web service.

Originally JAXB was to be used for the XML parsing during this sprint, but there were many naming conflicts in the generated code for the XML schemata that, given the time constraints, it was easier to write DOM parsers instead. After consulting with Snowflake, it was discovered that they too attempted to use JAXB but abandoned it due to the same issue.

\subsubsection{Review}

The majority of the goals for this sprint were completed, and two projects were produced - the AES service itself, consisting of the producer and WS-Notification broker implementation, and a separate project containing an example consumer. There was not enough time to implement temporal filtering in WS-Notification for the demonstration to the customer. However the code was partially implemented in the back-end of the system as the OpenGIS standard already has fields for temporal filtering this extension is absolutely possible.

\subsection{Final Meeting}
\label{sec:impl_final_meeting}

The final meeting consisted of the group demoing and explaining the finished project (i.e. the results of sprint 5), not just to the usual customer representatives, but to a large number of other Snowflake employees that had no previous involvement with or knowledge of the project (including the CEO and a number of both technical and non-technical roles).

The demo of the software showed multiple consumers receiving NOTAMs sent by a single producer. The consumers subscribed to a broker of WS-Notifications using OpenGIS spatial filters. The spatial filter was made of an axis-aligned bounding box that matched airports based on the geographic data in the NOTAMs which had been included by the enrichment code.

Once the demo was concluded, the group fielded questions, both from the original customer representatives about the progress in the last sprint, and from those new to the project about the project as a whole. The general architecture of the system was explained, as shown in figure~\ref{fig:system_overview}.

\section{Final System Overview}
\label{sec:final_system}

\begin{figure}
\begin{tikzpicture}[scale=0.75, every node/.style={transform shape}, node distance=2cm]
     \node (Producer1) [abstract, rectangle split, rectangle split parts=2]
        {
            \textbf{Producer}
            \nodepart{second}JMS Publisher
        };

    \node (ActiveMQ) [abstract, rectangle split, rectangle split parts=1, right=5cm of Producer1]
        {
            \textbf{ActiveMQ}
        };

    \node (Producer2) [abstract, rectangle split, rectangle split parts=2, right=5cm of ActiveMQ]
    {
        \textbf{Producer}
        \nodepart{second}JMS Publisher
    };

    \node (WSN Broker1) [abstract, rectangle split, rectangle split parts=1, below left=0.8cm of ActiveMQ]
        {
            \textbf{WSN Broker}
        };

    \node (WSN Broker2) [abstract, rectangle split, rectangle split parts=1, below right=0.8cm of ActiveMQ]
        {
            \textbf{WSN Broker}
        };

    \node (Consumer) [abstract, rectangle split, rectangle split parts=2, below right=0.8cm of WSN Broker1]
    {
        \textbf{Consumer}
        \nodepart{second}WSN Consumer
    };

    \node (Consumer1) [abstract, rectangle split, rectangle split parts=2, below left=0.8cm of WSN Broker1]
    {
        \textbf{Consumer}
        \nodepart{second}WSN Consumer
    };

    \node (Consumer2) [abstract, rectangle split, rectangle split parts=2, below=1cm of WSN Broker2]
    {
        \textbf{Consumer}
        \nodepart{second}WSN Consumer
    };

    \draw[-latex] (Producer1.east) -- (ActiveMQ.west) node[midway,above] {JMS Message Publish};
    \draw[-latex] (Producer2.west) -- (ActiveMQ.east) node[midway,above] {JMS Message Publish};
    \draw[-latex] (WSN Broker1.north) -- (ActiveMQ.south) node[midway,left] {JMS Subscription};
    \draw[-latex] (WSN Broker2.north) -- (ActiveMQ.south) node[midway,right] {JMS Subscription};
    \draw[-latex] (Consumer.north) -- (WSN Broker1.south) node[midway,right] {};
    \draw[-latex] (Consumer1.north) -- (WSN Broker1.south) node[midway,right] {};
    \draw[-latex] (Consumer2.north) -- (WSN Broker2.south) node[midway,right] {WSN(SOAP over HTTP)};

\end{tikzpicture}
\caption{System overview}
\label{fig:system_overview}
\end{figure}

The completed system consists of two parts: a producer, which parses NOTAM data and publishes it to a JMS broker (ActiveMQ, in this project), and a basic WS-BrokeredNotification implementation that allows consumers to retrieve messages from this JMS broker via the WS-Notification protocol. A block diagram giving an overview of this system is provided in figure~\ref{fig:system_overview}.

The system was designed with scalability in mind, since it needed to be able to handle a large volume of NOTAM traffic, and a potentially large number of consumers. For this reason, an arbitrary number of producers and an arbitrary number of WS-Notification brokers are supported, with each broker servicing multiple consumers. The limiting factor in this setup is the number of simultaneous connections that the JMS broker itself is able to service. Since ActiveMQ already has robust support for clustering, this was deemed to be a non-issue.

%REPLACE FIGURE X BELOW
Although the consumer was out of this project's scope, a basic implementation was still produced, in order to demonstrate the system's functionality. A class diagram for this separate project is given in figure X. Note that this project does not directly reference any of the classes from the main project; instead, it uses the published WSDL for the project's service, along with JAX-WS, to connect to the service.

\section{Critical Evaluation}
\label{sec:critical_evaluation}

In order to properly judge the success of a project such as this, it is important to compare the finished product to the requirements and goals set out initially. This can be somewhat difficult when Scrum is used to manage a project, given how goals and requirements are subject to change each sprint. Thankfully, the overall scope of the project only received minor alterations, so almost all of the requirements initially set out by the customer remain relevant to the final product. Every requirement was met, with come concessions made with respect to features such as complex spatial filtering in the interest of time and given the fact that the product is a proof of concept for future works rather than a product to be used by flight planners.

\subsection{Project management}
\label{sec:eval_project_management}

The project used an iterative process of constant performance assessment and improvement of project management. Each biweekly sprint meeting had time dedicated to project performance analysis, to assess how well the team was working together and to find ways the processes and group dynamics could be improved. This included anonymous ratings of how comfortable team members felt expressing their true opinions (from 1 to 5), to get a sense of how accurately group members were expressing their opinions on the project and to make sure they weren't harbouring grievances that could affect project performance and felt they weren't able to express. This seemed to be a successful technique as overall ratings gradually climbed throughout the project and a number of issues were identified and solved in these meetings. The customer also took part in this activity, so we the group could be sure their feedback was genuine and unmollified.

These meetings also included a 'happiness breakdown' chart, getting individual group members to identify areas that they felt worked well, areas of ambivalence and areas that they felt needed improvement; the areas were then diplomatically prioritised for discussion. The customer also took part in this activity, helping to ensure that they were not only satisfied with the results and what was being developed, but also with the way in which it was being developed. This method of project performance analysis was particularly effective and helped to solve many early problems, as well as ensuring practices that were working well were maintained. One problem in particular it identified was the high frequency of meetings during the first sprint, as well as these meetings being scheduled without enough advanced notice and without checking it worked with everyone's schedule. As a result of the sprint 2 planning meeting, meetings were then confined to two regularly scheduled meetings per week, that were checked to be at a convenient time for all group members. This process also motivated the change from issue tracking through JIRA to solely using GitHub, as several group members thought using JIRA was unnecessarily cumbersome, time consuming, and hindered development.

Another successful area of project management was issue tracking, once the change had been made to being solely reliant on GitHub. The process of adding all issues to the repository on the same day as they were brought up in the sprint meetings ensured issues weren't forgotten at any point or neglected to potentially be rediscovered when it was too late to deal with (or worse yet, not discovered at all). It also allowed the customer to keep track of progress and be notified of new issues that arise, not just during sprint meetings, but throughout the week. The use of GitHub also allowed issues to be tied to specific commits, so which commit solved a particular issue could be kept track of, indicating where to roll back to if the feature were to be broken in a later commit, or where to scrutinise if another issue with the feature arises. Although one downside of using GitHub is that it doesn't provide burndown charts or other traditional project management tools. However, when combined with the regular sprint planning meetings, it was felt that this weakness of GitHub didn't affect the project performance.

The use of GitHub and the particular git work flow also ensured there was an effective code review process, as code reviews were mandated before a feature would be merged into the master branch. This slows down the integration of new features but the effects are well worth the bureaucratic overhead. This process of code review helped to catch bugs before they were introduced to the main program, helped to ensure programming consistency across group members (such as ensuring consistent forms of code documentation and explanation, and consistent coding style) and helped to build team cohesion, ensuring every member of the team knows what every other member has written and understands how it is implemented.

Despite these successes, there were some areas that could have been improved. Intergroup communication outside of regular meetings could have been improved, with some members meeting outside of regularly scheduled meetings without effectively conveying the results of those meetings to all the team, resulting in some wasted or duplicated work, with the decision being made to abandon certain technologies whilst certain members continued to work with them. Meetings could also have been better organised, with a formal written agenda, to ensure they are focused and productive, and to cut down on the number of meetings were it was realised the topic of discussion could have been resolved more quickly and easily remotely, saving valuable time. They could have also benefited from having a formally designated chair to also keep the meetings focused, and from a  minute taker, so fewer details would be forgotten and the decisions made in meetings could be easily passed on to anyone not in attendance.

Another major problem was with the one sided work allocation, with a very large portion of work being carried out in small groups, rather than following usual Scrum practises with regular stand up meetings. However, it is difficult to see how this could have been rectified without the benefit of hindsight, given the exploratory nature of the project and the somewhat monotonic project progress, where few things could be done concurrently and had to be done in a strict order.

\subsection{Customer feedback}
\label{sec:eval_customer_feedback}

In addition to the feedback given in sprint planning meetings throughout the project, and the feedback given in the final meeting, at the group's request, Snowflake sent an email with some feedback and reflections on the project performance and outcome, as shown in the appendix.

The feedback was mostly positive, particularly focusing on the exploratory goals of the project, i.e., the successful evaluation of the usefulness of a large number of technologies to the project (the as a whole, including the continued development by Snowflake, rather than just limited to the scope of the GDP work). Even though it was determined that most of those technologies were wanting, it was not wasted effort, since had it not been done, Snowflake would have had to have gone through the same exploratory process.

\blockquote{...good outcome for Snowflake Software; they answered some of our key questions and provided a solid foundation to commence our Pub-Sub product development in 2015... Reaching firm conclusions about the relative merits of the technologies you assessed represents a real achievement.}

Their main criticism was the one-sidedness of the sprint planning meetings and of the final meeting, where a large proportion of the speaking and demonstration was done by James.

\section{Conclusions}
\label{sec:conclusions}

In this report the design, development process and testing of the AES system has been recounted. The final product is a notification system for aeronautic standard messages known as NOTAMs, it utilises a publish/subscribe architecture with brokers handling messages between producers and the consumers. The product handles multiple types of message filtering, allowing consumers to subscribe based on simple text, locations within bounding boxes and within a given time period.

As stated as a requirement, AES makes use of the WS-Notification standard for communication between brokers and consumers. This standard was chosen due to its common usage within the aeronautics industry for communication. Messaging done between brokers, the ActiveMQ message queue and the producers uses the Java Messaging Service.

The AES system was tested to handle multiple publishers and consumers, however its limits were not stress tested. Given the extensible nature of its design it should be able to handle any number of producers, consumers or NOTAMs within reason, but due to time and monetary constraints it was not possible to confirm this.

A main limitation of the final product is its limited implementations of WS-Notifications and OpenGIS filtering standards, only including a subset of directly relevant components. The decisions to only implement the parts of the standard which were vital to the project were made in the interest of time. If AES was built upon, completing these implementations would be required to properly adhere to the standards.

An initially important goal of the project which had to be abandoned fairly early on was polling of the Federal NOTAM System. As the aim was to produce a system which took NOTAMs from their source, filtered them and communicated them to the correct consumers, this was to be an important component of the project. Unfortunately this was deemed impossible as access to the FNS is restricted and requires registration before you are authorised. This was deemed to be outside of the scope of a proof of concept design.

\newpage

\section{Future Work}
\label{sec:future_work}

\subsection{Performance/Scalability Testing}
\label{sec:future_testing}

Although the system was designed to scale well with available hardware, it was not possible to test this functionality within the time constraints of the project. Given more time, the system's scalability could be tested by launching multiple instances of the WSN broker on Amazon EC2 instances, and measuring both the number of simultaneous subscriptions each is able to request, and the total number of simultaneous subscriptions that the entire system is able to service. The limiting factor for the latter will be the throughput of the JMS broker (ActiveMQ). This testing would require additional funding to pay for the required EC2 instances.

Alternatively, the system could be deployed on Amazon Elastic Beanstalk, which automatically scales the service by provisioning additional EC2 instances when under high load. An Elastic Beanstalk distribution could be created for the service, and the peak number of subscriptions that the system is able to service could be measured. Again, this testing would require additional funding and by the time it was realised that this type of testing would be useful to the project it was not possible to request funding.

\subsection{IATA Designators and ICAO Location Indicators}
\label{sec:future_designators}

Global Aerodromes is a service provided by Snowflake which allowed the AES system to map from aerodromes referenced in NOTAMs to GPS co-ordinates, a feature required in order to perform spatial filtering. Within the AIXM specification for NOTAMs, both IATA (International Air Transport Association) designators and ICAO (International Civil Aviation Organisation) location indicators are specified. Both of these designating IDs are used by Global Aerodromes and the AES service produced made use of just IATA designators to pair NOTAMs to GPS co-ordinates. This caused an issue due to a fraction of the aerodromes listed by the Global Aerodromes service not containing a value for this identifier. A possible future extension would be to add a contingency for this occurrence and include support for ICAO location indicators as well, to maximise the chance that every NOTAMs aerodrome is matched to a location for filtering.

\subsection{Compile Time Issues}
\label{sec:future_compilation}

A major issue with the final version of the AES system is compile time problems. The reason for these issues is the time required to download all required XML documents and WSDL files. The WSDL files in particular require a large number of external resources including more WSDL files and relevant XSD schemata, each of these referencing even more resources in a long chain. A number of these chained references take a long time to download and as they are referenced multiple times from different locations they are downloaded multiple times, greatly increasing compile times.

A solution to this would be the implementation of XML catalogues which map external references to previously downloaded local files. This would dramatically reduce compile times at the expense of file size, however if the product is to be extended then the time saved would be well worth the cost.

This was not previously implemented due to the team's inexperience at working with them and the compile time creep did not occur until late in the project. At which point a cost-benefit analysis of learning how to use this tool and implementing it into the system at that late a stage was done and decided against.

\newpage

\addcontentsline{toc}{section}{References}
\sloppy
\printbibliography

\appendix

\section{Report authorship}

\begin{tabularx}{\textwidth}{|X|X|} \hline
  \emph{Section}                         & \emph{Author}                      \\ \hline
  Abstract                               & Peter Prince                       \\ \hline
  \fullref{sec:terminology}              & Peter Prince                       \\ \hline
  \fullref{sec:introduction}             & Charles Sherman\par Peter Prince   \\ \hline
  \fullref{sec:intro_motivation}         & Peter Prince                       \\
  \fullref{sec:intro_customer}           & Peter Prince\par Andrew Sullivan   \\
  \fullref{sec:intro_goals}              & Peter Prince                       \\ \hline
  \fullref{sec:bg_notams}                & Peter Prince\par Andrew Sullivan   \\
  \fullref{sec:bg_pubsub}                & James Robinson                     \\
  \fullref{sec:bg_wsn}                   & Peter Prince                       \\
  \fullref{sec:bg_wsdl}                  & Andrew Sullivan                    \\
  \fullref{sec:bg_parsing}               & Peter Prince                       \\
  \fullref{sec:bg_catalogues}            & Peter Prince                       \\
  \fullref{sec:bg_jira}                  & Peter Prince                       \\
  \fullref{sec:bg_activemq}              & Peter Prince                       \\
  \fullref{sec:bg_git}                   & Andrew Sullivan                    \\
  \fullref{sec:bg_travis}                & Andrew Sullivan                    \\
  \fullref{sec:bg_cucumber}              & Andrew Sullivan                    \\
  \fullref{sec:bg_soap}                  & Andrew Sullivan                    \\
  \fullref{sec:bg_netcat}                & Andrew Sullivan                    \\
  \fullref{sec:bg_curl}                  & Andrew Sullivan                    \\
  \fullref{sec:bg_ogc}                  & Andrew Sullivan                    \\
  \fullref{sec:bg_gml}                  & Andrew Sullivan                    \\
  \fullref{sec:bg_wfs}                   & Andrew Sullivan                                       \\ \hline
  \fullref{sec:planning}                 & Peter Prince                       \\
  \fullref{sec:plan_skills_audit}        & Peter Prince                       \\
  \fullref{sec:plan_risk_analysis}       & Charles Sherman                    \\
  \fullref{sec:plan_customer_relations}  & Peter Prince                       \\
  \fullref{sec:plan_deliverables}        & Peter Prince                       \\
  \fullref{sec:version_control}          & James Robinson                     \\
  \fullref{sec:plan_testing_methodology} & Andrew Sullivan                    \\ \hline
\end{tabularx}
\newpage
\begin{tabularx}{\textwidth}{|X|X|} \hline
  \emph{Section}                         & \emph{Author}                      \\ \hline
  \fullref{sec:implementation}           & Charles Sherman                    \\
  \fullref{sec:impl_sprint_1}            & Charles Sherman\par Peter Prince   \\
  \fullref{sec:impl_sprint_2}            & Charles Sherman\par Peter Prince   \\
  \fullref{sec:impl_sprint_3}            & Charles Sherman\par Peter Prince   \\
  \fullref{sec:impl_sprint_4}            & Charles Sherman\par Peter Prince   \\
  \fullref{sec:impl_sprint_5}            & Charles Sherman\par James Robinson \\
  \fullref{sec:impl_final_meeting}       & Andrew Sullivan                    \\ \hline
  \fullref{sec:final_system}             & James Robinson\par Charles Sherman \\ \hline
  \fullref{sec:critical_evaluation}      & Peter Prince                       \\
  \fullref{sec:eval_project_management}  & Andrew Sullivan                                  \\
  \fullref{sec:eval_customer_feedback}   & Andrew Sullivan                    \\ \hline
  \fullref{sec:conclusions}              & Peter Prince                       \\ \hline
  \fullref{sec:future_testing}           & James Robinson                     \\
  \fullref{sec:future_designators}       & Peter Prince                       \\
  \fullref{sec:future_compilation}       & Peter Prince                       \\ \hline
\end{tabularx}

\section{Project Specification}


\begin{center}
\includegraphics[scale=0.7]{spec.pdf}
\end{center}


\begin{center}
\includegraphics[scale=0.8]{GanttChart.pdf}
\end{center}

\section{Feedback email from Snowflake Software}

Hi guys\\
As discussed, some feedback for you from Eddie and myself.
\\\\
The deliverables produced from your Group Design Project have resulted in a good outcome for Snowflake Software; they answered some of our key questions and provided a solid foundation to commence our Pub-Sub product development in 2015.
\\\\
The project posed some significant challenges including reconciling complex technical standards with the user requirements and a complex set of factors to weigh up in selecting the technologies to use. Reaching firm conclusions about the relative merits of the technologies you assessed represents a real achievement.
\\\\
As mentioned during the wrap up from the final demonstration I feel it would have been productive if there was a more balanced contribution to Sprint planning, demonstration and retrospectives from each member of the team.
\\\\
If possible we would like a copy of your group report and individual reflections to further understand your view of what happened within the project, so that we can improve the experience for next year's group.
\\\\
We enjoyed collaborating with you and are hopeful that it was a positive experience that has helped you understand something of the software industry.
\\\\
If any of you are interested in exploring the possibility of joining us at Snowflake Software I would be more than happy to catch-up for an informal conversation.
\\\\
Good luck for the end of your degree and future career.
\\\\
Adrian\\
-- \\
Adrian McKenzie\\
Head of Software Development\\
Snowflake Software


\end{document}
